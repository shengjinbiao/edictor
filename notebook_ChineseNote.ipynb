{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Comparison with LingPy\n",
    "\n",
    "# 用LingPy进行序列比较\n",
    "\n",
    "LingPy 是一套开源 Python 模块，用于定量历史语言学中的序列比较、距离分析、数据操作和可视化方法。\n",
    "\n",
    "基本上，本教程假设您至少具备本科水平的历史语言学理解（特别是历史语言比较中使用的基本方法，通常归纳为“比较法”），仅需要具备Python和命令行操作的基本知识。\n",
    "\n",
    "该教程分为以下几个部分：\n",
    "\n",
    "1. 数据操作：准备、加载和测试词表数据\n",
    "2. 语音对齐\n",
    "3. 自动同源词检测\n",
    "4. 评估\n",
    "5. 数据导出\n",
    "\n",
    "This tutorial will run you through the major steps needed in order to infer cognates automatically with [LingPy](http://lingpy.org) ([List and Forkel 2017](http://bibliography.lingpy.org?key=List2017i)) from linguistic word list data and to export the data into various formats so that you can either inspect them using tools like the [EDICTOR](http://edictor.digling.org) ([List 2017](http://bibliography.lingpy.org?key=List2017d)), or further analyse them, using software like [SplitsTree](http://splitstree.org) ([Huson 1998](http://bibliography.lingpy.org?key=Huson1998)) or [BEAST](http://www.beast2.org) ([Bouckaert et al. 2014](http://bibliography.lingpy.org?key=Bouckaert2014)).\n",
    "\n",
    "Basically, this tutorial assumes that you have at least an undergraduate level understanding of historical linguistics (particularly the basic methods used in historical language comparison, often summarized under the label \"comparative method\"), requiring only working knowledge of Python and command line operation.\n",
    "\n",
    "It is required that you have installed both LingPy in the version [2.6](https://github.com/lingpy/lingpy/releases/tag/v2.6) for Python3 (as this tutorial will assume that you use Python3) available on [GitHub](https://github.com/lingpy/lingpy), and (as a plus) the [python-igraph](http://igraph.org) package ([Csárdi and Nepusz 2006](http://bibliography.lingpy.org?key=Csardi2006)). Furthermore, in order to follow all examples given in this tutorial, it is useful to work with the [IPython](http://ipython.org) suite, which is very convenient for testing code pieces directly against the Python interpreter.\n",
    "\n",
    "The tutorial is divided into different blocks, during which different aspects of sequence comparison will be illustrated from the concrete perspective of LingPy. In order to understand fully all that is going on, however, this tutorial won't be sufficient, and it is recommended that those who are interested in the algorithmic and conceptual details of LingPy's major algorithms for sequence comparison have a closer look at the book [Sequence Comparison in Historical Linguistics](https://sequencecomparison.github.io) ([List 2014](http://bibliography.lingpy.org?key=List2014d)) in which the most comprehensive state of the art is reflected. More recent papers might occasionally be mentioned in order to account for those aspects of sequence comparison which have been changed since then, but the book on sequence comparison (which is also freely available for download) is still the best starting point.\n",
    "\n",
    "The tutorial is divided into the following parts:\n",
    "\n",
    "1. Hands on the Data: Preparing, Loading, and Testing Word List Data\n",
    "2. Phonetic Alignment\n",
    "3. Automatic Cognate Detection\n",
    "4. Evaluation\n",
    "5. Exporting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Installation Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Confirming that Python and LingPy are not installed\n",
    "\n",
    "In order to proceed with LingPy installation, make sure you have at least a standard Python interpreter installed. Open a command prompt and run the command below, which, if Python is installed, will return a message informing the version of Python that is installed (something similar to `\"Python 2.7.12\"`).\n",
    "\n",
    "```shell\n",
    "$ python --version\n",
    "```\n",
    "\n",
    "In case you are using or planning to use Python 3, the command below will return something like `\"Python 3.5.2\"`.\n",
    "\n",
    "```shell\n",
    "$ python3 --version\n",
    "```\n",
    "\n",
    "To verify if LingPy is installed you can either run your Python interpreter and try to load the library with `import lingpy` (which will return an `ImportError` if the library is missing) or copy and paste the command below to your command prompt (changing `python` to `python3` if needed).\n",
    "\n",
    "```shell\n",
    "$ python -c \"import pkgutil; print('LingPy is installed' if pkgutil.find_loader('lingpy') else 'LingPy is not installed')\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LingPy is installed\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "print('LingPy is installed' if importlib.util.find_spec('lingpy') else 'LingPy is not installed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Updating and preparing the system\n",
    "\n",
    "Before proceeding with LingPy installation, make sure your system is up to date. Here we reproduce the commands for the most common Linux distributions, but most of them have similar or equivalent commands.\n",
    "\n",
    "```shell\n",
    "sudo apt update && sudo apt upgrade # for Debian/Ubuntu Systems\n",
    "yum update # for Fedora\n",
    "pacman -Syu # for Arch systems\n",
    "```\n",
    "\n",
    "Following the best practices of Python development, we recommend you install LingPy and all its dependencies with the `pip` package manager. `pip` is included in all recent version of Python, but your distribution might have removed it. You can check if `pip` is installed by running the command below, which should return a message like `pip 9.0.1 from /usr/local/lib/python3.5/dist-packages (python 3.5)` (depending on your setup, the command might be `pip3` for Python 3).\n",
    "\n",
    "```shell\n",
    "$ pip --version\n",
    "```\n",
    "\n",
    "If `pip` is not installed, we recommended that you [properly install `pip` according to its instructions](https://pip.pypa.io/en/stable/installing/) instead of relying on your distribution repositories, which might be outdated and can lead to errors and conflicts. If you want to use Python3, but have also Python2 installed (which is the norm), make sure that `pip` is pointing to the correct Python version (on some machines, the Python3 version is, for example, called `pip3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Installing LingPy\n",
    "\n",
    "The LingPy library can be installed either by using the packaged version on the Python Package Index (PyPI) or by making a full local copy of the development file on GitHub (some information can also be found at the [LingPy documentation page](http://lingpy.org/tutorial/installation.html)). The first alternative is recommended if you are new to Python and to LingPy, while the second is the best choice if you plan to contribute to LingPy development or alter it (in which case, however, you should be proficient enough with `git` to fork or branch the repository and should probably use a virtual environment). In any case, both alternatives work in the same way for the purposes of this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Installing from PyPi\n",
    "\n",
    "The simplest alternative is to install LingPy using the standard software repositories (with `pip`) and installing locally (in the user directory, without superuser permission, with the `--user` flag) with the following command:\n",
    "\n",
    "```shell\n",
    "pip install --user lingpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Installing from git\n",
    "\n",
    "When installing the development version (which most users won't need, we only mention it here for completeness) you will locally clone the `git` repositories and instruct `pip` to use the local copy as source, so that any changes to the code can immediately be used without having to package lingpy or submit a pull request to the authors. You must make sure you have `git` properly installed by running the `git --version` command and installing `git` if needed; in some systems, you will probably also need some development tools and libraries. For Debian/Ubuntu systems, everything should be installed with the command below:\n",
    "\n",
    "```shell\n",
    "sudo apt install git build-essential python-dev python3-dev\n",
    "```\n",
    "\n",
    "When all dependencies are installed, you can clone lingpy's repository and install it with `pip` in development mode:\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/lingpy/lingpy.git\n",
    "pip install --user -e lingpy\n",
    "```\n",
    "\n",
    "This command might take some time to finish, as it might need to download and install dependencies such as numpy, appdirs, etc. (those are listed in the `requirements.txt` file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configuring LingPy\n",
    "\n",
    "If LingPy was not installed, it is recommended you import it one first time from the command line, so that it can configure itself and preprocess some data, such as sound profiles, graphemes, etc. Just enter the command below in your command line, which can result on a long list of information on the configuration phase; once it is finished, you should be ready to proceed with the tutorial:\n",
    "\n",
    "```shell\n",
    "$ python -c \"import lingpy\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Installing additional software required for this tutorial\n",
    "\n",
    "For all scripts in this tutorial to run, a couple of additional software tools will need to be installed. For automatic segmentation, you will need to install the *segments* package:\n",
    "\n",
    "* https://github.com/bambooforest/segments/\n",
    "\n",
    "This package can either be installed via git (from GitHub), or via PyPi.\n",
    "\n",
    "For the Infomap cluster analyses, you will need the python-igraph package. If you want to test these analyses, please visit their homepage at [http://igraph.org/](http://igraph.org/) and see for further installation instructions, as they may vary from machine to machine, and they offer an excellent support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Additional software, not required, but recommended\n",
    "\n",
    "While not required, installing [IPython](http://ipython.org) should improve your experience when following this tutorial and with LingPy in general. Furthermore, when installing the whole [jupyter](http://jupyter.org/) suite, you will be able to launch this tutorial and follow all steps interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Hands on the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Testset\n",
    "\n",
    "Linguists are often skeptical when they hear that LingPy requires explicit phonetic transcriptions, and often, they are even reluctant to interpret their data along the lines of the International Phonetic Alphabet. But in order to give the algorithms a fair chance to interpret the data in the same way in which they would be interpreted by linguists, a general practice for phonetic transcriptions is indispensable. \n",
    "\n",
    "For our test, we will use a dataset consisting of 30 Polynesian languages taken from the [ABVD](http://language.psy.auckland.ac.nz/austronesian/) ([Greenhill et al. 2008](http://bibliography.lingpy.org?key=Greenhill2008)). This dataset was intensively verified and revised according to primary source data from widely accepted published works (dictionaries and online databases) and first-hand knowledge of a Polynesian linguist. Revisions included: correcting invalid entries, reordering misplaced segments, adding missing segments, modifying ambiguous transcription conventions, and deleting semantically distant forms for concepts with multiple entries. The dataset was additionally cleaned by converting the various original transcriptions into a valid version of IPA accepted by LingPy (for details, see 2.4 below). The testset is located in the same folder in which you also find this interactive tutorial, which we provide in various formats. In the following, we will assume that you opened the terminal in this folder (or ``cd``ed into this folder after opening your terminal). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Input Format\n",
    "\n",
    "Let us start by quickly examining the file `polynesian.tsv` which we prepared for this occasion. This file is a tab-separated text file with the first row indicating the header, and the very first column is reserved for numeric identifiers. If you open this file in a spreadsheet editor (and make sure to select \"tab\" as a delimiter, and NO characters to delimit a cell), will see that it is a very straightforward spreadsheet, in which the first row is a header indicating the names of the columns, and the first cell is reserved for an identifier, which should be numeric (but order does not matter).\n",
    "\n",
    "|   ID | DOCULECT        | CONCEPT   | GLOTTOCODE   |   CONCEPTICON_ID | VALUE   | FORM   | TOKENS   | VARIANTS   | SOURCE                 |   COGID |\n",
    "|-----:|:----------------|:----------|:-------------|-----------------:|:--------|:-------|:---------|:-----------|:-----------------------|--------:|\n",
    "| 7435 | Anuta           | Eight     | anut1237     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 3591 | East_Futunan    | Eight     | east2447     |             1705 | valu    | valu   | v a l u  | valu       | POLLEX                 |     663 |\n",
    "| 1401 | Emae            | Eight     | emae1237     |             1705 | βaru    | βaru   | β a r u  |            | 52375                  |     663 |\n",
    "| 5359 | Futuna_Aniwa    | Eight     | futu1245     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 5614 | Hawaiian        | Eight     | hawa1245     |             1705 | walu    | walu   | w a l u  |            | 71458                  |     663 |\n",
    "|  949 | Kapingamarangi  | Eight     | kapi1249     |             1705 | walu    | walu   | w a l u  | waru       | POLLEX                 |     663 |\n",
    "| 2114 | Luangiua        | Eight     | onto1237     |             1705 | valu    | valu   | v a l u  |            | POLLEX                 |     663 |\n",
    "| 1853 | Mangareva       | Eight     | mang1401     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "|  725 | Maori           | Eight     | maor1246     |             1705 | waru    | waru   | w a r u  |            | Biggs-85-2005          |     663 |\n",
    "| 4201 | Mele_Fila       | Eight     | mele1250     |             1705 | eβaru   | eβaru  | β a r u  |            | 52375                  |     663 |\n",
    "| 4855 | Niuean          | Eight     | niue1239     |             1705 | valu    | valu   | v a l u  |            | POLLEX                 |     663 |\n",
    "| 3297 | North_Marquesan | Eight     | nort2845     |             1705 | va'u    | va'u   | v a ʔ u  |            | POLLEX                 |     663 |\n",
    "| 6362 | Nukuria         | Eight     | nuku1259     |             1705 | varu    | varu   | v a r u  |            | Davletshin-1212-2015   |     663 |\n",
    "| 6152 | Penryhn         | Eight     | penr1237     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 3897 | Pukapuka        | Eight     | puka1242     |             1705 | valu    | valu   | v a l u  |            | Salisbury-152-2005     |     663 |\n",
    "| 1595 | Rapanui         | Eight     | rapa1244     |             1705 | va'u    | va'u   | v a ʔ u  |            | POLLEX                 |     663 |\n",
    "| 5850 | Rarotongan      | Eight     | raro1241     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 4395 | Ra’ivavae       | Eight     | aust1304     |             1705 | vaGu    | vaGu   | v a ɢ u  |            | Tamaititahio-1213-2015 |     663 |\n",
    "| 6914 | Rennell_Bellona | Eight     | renn1242     |             1705 | baŋgu   | baŋgu  | b a ŋg u |            | POLLEX                 |     663 |\n",
    "| 5101 | Rurutuan        | Eight     | aust1304     |             1705 | vaʔu    | vaʔu   | v a ʔ u  |            | Meyer-128-2005         |     663 |\n",
    "| 6623 | Samoan          | Eight     | samo1305     |             1705 | valu    | valu   | v a l u  |            | Blust-118-2005         |     663 |\n",
    "| 3076 | Sikaiana        | Eight     | sika1261     |             1705 | valu    | valu   | v a l u  |            | POLLEX                 |     663 |\n",
    "| 1169 | Tahitian        | Eight     | tahi1242     |             1705 | va'u    | va'u   | v a ʔ u  | varu       | Clark-173-2005         |     663 |\n",
    "| 2823 | Tikopia         | Eight     | tiko1237     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 2336 | Tongan          | Eight     | tong1325     |             1705 | valu    | valu   | v a l u  |            | 117207                 |     663 |\n",
    "| 4592 | Tuamotuan       | Eight     | tuam1242     |             1705 | varu    | varu   | v a r u  |            | POLLEX                 |     663 |\n",
    "| 7179 | Tuvalu          | Eight     | tuva1244     |             1705 | valu    | valu   | v a l u  |            | 29903                  |     663 |\n",
    "|    8 | Vaeakau_Taumako | Eight     | pile1238     |             1705 | valu    | valu   | v a l u  |            | Hovdhaugen-375-2009    |     663 |\n",
    "|  251 | Wallisian       | Eight     | wall1257     |             1705 | valu    | valu   | v a l u  |            | POLLEX                 |     663 |\n",
    "\n",
    "\n",
    "You may even prepare your data in a spreadsheet to then analyze it in LingPy. You just need to make sure to export it properly to the TSV format (which you can easily do by just copy-pasting it into an empty text-file). What you need to know about the format, however, is the following:\n",
    "\n",
    "1. contrary to most linguists' intuition, the columns do **not** indicate languages: each row indicates one word and, as a result, language names need to be redundantly repeated\n",
    "2. certain columns are **required** by LingPy, and their number can vary, depending on the task you want to carry out: for the purpose of cognate detection, you need at least the columns `doculect`, `concept`, and either a plain transcription (the default column name is `ipa`) or a more advanced and less ambiguous transcription in segmented form (the default column name is `tokens`).\n",
    "3. in order to increase readability, column headers are upper-case when LingPy writes them to file, but this is not required (internally all columns are represented as lowercase when loaded into LingPy's objects).\n",
    "4. depending on the names of the columns, values will be interpreted by default: if you have a column called `cogid`, this will be converted to an integer, and `tokens` usually assumes that you have a string separated by spaces. As a result, LingPy may throw an error if your columns do not follow these required formats. To check how columns are interpreted, you can check the file [wordlist.rc](https://github.com/lingpy/lingpy/blob/master/lingpy/data/conf/wordlist.rc) where you will find a full account of currently supported values.\n",
    "5. users can add as many columns as they want, provided the file header is in alphanumeric form, and we have used this to add a field called \"variants\" in which we included forms that are obvious pronunciation variants in our data but should not be regarded by the algorithm. \n",
    "\n",
    "Not all of the columns in the table above are fully \"standardized\". The `DOCULECT` one, for example, so far only requires that distinct languages are given distinct names, no matter what those names contain (as long it has no tabulation stops). But for the purpose of exporting the data to other formats afterward, it is useful to restrict to alphanumeric names here, and to exclude all brackets or spaces from the language names (as well as quotes and diacritics), as we have been doing in this test set. This becomes especially important when inferring trees or using trees in further LingPy analyses: as trees are represented in the [Newick](https://en.wikipedia.org/wiki/Newick_format) format, where brackets play an important role, brackets in the names for the doculects will confuse the algorithm and raise an error. \n",
    "\n",
    "As the last point, note that we list `GLOTTOCODE` and `CONCEPTICON_ID`, which follows two major requirements for word list data we try to establish for the [Cross-Linguistic Data Formats (CLDF)](http://cldf.clld.org) initiative. As the linguistic sign has three major dimensions, the *language*, the *meaning*, and the *word form*, `GLOTTOCODE`, the language identifier provided by the [Glottolog project](http://glottolog.org) ([Hammarström, Forkel and Haspelmath 2017](http://bibliography.lingpy.org?key=Hammarstroem2017)) and `CONCEPTICON_ID`, the meaning identifier provided by the [Concepticon project](http://concepticon.clld.org) ([List, Cysouw, and Forkel 2016](http://bibliography.lingpy.org?key=List2016a)) cover two of these aspects, while the third aspect, the consistency of the form, is currently covered by LingPy (more on this below).\n",
    "More information on the data formats employed in LingPy can be found at the official [documentation page](http://lingpy.org/tutorial/formats.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loading the Data into a `Wordlist` Object\n",
    "\n",
    "Loading the data into LingPy is straightforward. LingPy has a couple of classes which are specifically designed to handle word list data, and these classes provide all similar basic functions, plus additional ones for specific purposes:\n",
    "\n",
    "* `Wordlist`: Basic class with core functionality, allows to read, modify, and write word list data, also allows  calculating distance matrices from cognate sets as well as rudimentary tree reconstruction functions (UPGMA, [Sokal and Michener 1958](http://bibliography.lingpy.org?key=Sokal1958), Neighbor-joining, [Saitou and Nei 1987](http://bibliography.lingpy.org?key=Saitou1987)).\n",
    "* `Alignments`: Class allows to align all cognate sets in a word list. Requires one column which stores the cognate sets as well as a column for `doculect`, `concept`, and transcription (default: `ipa`) or user-defined *segmented transcription* (default: `tokens`). Alignments can be carried out in different ways, the algorithms follow the ones first described in [List (2012a)](http://bibliography.lingpy.org?key=List2012b).\n",
    "* `LexStat`: Core class for automatic cognate detection, following the algorithm first described in [List (2012b)](http://bibliography.lingpy.org?key=List2012a) and later expanded in [List (2014)](http://bibliography.lingpy.org?key=List2014d), and [List, Greenhill, and Gray (2017)](List2017c). \n",
    "* `Partial`: Recent algorithm proposed in [List, Lopez, and Bapteste (2016)](http://bibliography.lingpy.org?key=List2016g), allows -- provided data is morpheme-segmented -- to search for partial cognates in the data.\n",
    "\n",
    "We will start with the basic `Wordlist` object to illustrate some core facilities below. Let us thus quickly load some data into the Wordlist. We start with our file ```polynesian.tsv```:\n",
    "\n",
    "加载TSV词表或字表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordlist has 236 languages and 300 concepts across 577 rows.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from lingpy import *\n",
    "\n",
    "# load the wordlist\n",
    "#wl = Wordlist('polynesian.tsv')\n",
    "wl2=Wordlist(r'D:\\edictor\\tests\\data\\head_Cognates.tsv')\n",
    "# count number of languages, number of rows, number of concepts\n",
    "print(\n",
    "    \"Wordlist has {0} languages and {1} concepts across {2} rows.\".format(\n",
    "        wl2.width, wl2.height, len(wl2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By accessing the attributes `width` we retrieve the number of languages and with `height` we retrieve the number of concepts. This follows the logic inherent in the traditional format in which linguists prepare their spreadsheets, namely by placing concepts in the first column and languages in the rest of the columns. Traditional linguistic databases would thus represent the data from the table above as follows:\n",
    "\n",
    "CONCEPT | Emae | Rennell_Bellona | Tuvalu | Sikaiana | Penrhyn | Kapingamarangi\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "one | tasi| tahi | tahi | tasi | tahi | dahi\n",
    "five | rima | gima | lima | lima | rima | lima\n",
    "eight | βaru | baŋu | valu | valu | varu | waru\n",
    "... | ... | ... | ... | ... | ... | ...\n",
    "\n",
    "The disadvantage of this annotation is, however, that we can only store one value in each cell, and we will create inconsistencies if we try to mix information per cell. For that reason, we maintain the strict tabular representation where each word is placed in one row, but internally, LingPy represents the data in multidimensional tables in which languages are thought to be placed in the columns and concepts in the rows. \n",
    "\n",
    "There are multiple ways in LingPy to inspect and manipulate data using the `Wordlist` class, but it would go too far to mention them all here, so we will restrict it to one example, by which we retrieve the values from the six languages above for the entry \"Eight\", using the `wordlist.get_dict()` function, and refer the users to a longer tutorial which is [online](http://lingpy.org/tutorial/lingpy.basic.wordlist.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aklanon             \túeo(h)\n",
      "Amblau              \tolo\n",
      "Amblong             \tbatu-\n",
      "Amis                \tfoŋoh, foŋoh\n",
      "Anuta               \turu\n"
     ]
    }
   ],
   "source": [
    "# 打印出“头”这个概念的所有词形'\n",
    "\n",
    "head = wl2.get_dict(row='头', entry='FORM')\n",
    "\n",
    "# 只取有词形的前 5 个\n",
    "non_empty = [(lang, forms) for lang, forms in sorted(head.items()) if forms]\n",
    "for lang, forms in non_empty[:5]:\n",
    "    print(f\"{lang:20}\\t{', '.join(forms)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emae                   \t βaru\n",
      "Rennell_Bellona        \t baŋgu\n",
      "Tuvalu                 \t valu\n",
      "Sikaiana               \t valu\n",
      "Penryhn                \t varu\n",
      "Kapingamarangi         \t walu\n"
     ]
    }
   ],
   "source": [
    "# get all indices for concept \"eight\", `row` refers to the concepts here, while `col` refers to languages\n",
    "eight = wl.get_dict(row='Eight', entry='value')\n",
    "for taxon in ['Emae', 'Rennell_Bellona', \n",
    "              'Tuvalu', 'Sikaiana', 'Penryhn',  'Kapingamarangi']:\n",
    "    print(\n",
    "        '{0:20}'.format(taxon), '  \\t', ', '.join(eight[taxon]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordlists are the core format of data representation in LingPy, and it is useful for users who want to make advanced use of the library, to acquaintain them more with the way the data is handled here. More information can be found in the [online documentation](http://lingpy.org/tutorial/lingpy.basic.wordlist.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Segmentation of Phonetic Entries\n",
    "\n",
    "Converting transcription conventions into IPA is critical for ensuring that the dataset can be properly analysed. In the original Polynesian dataset, for example, multiple inconsistent conventions were discovered. These included outdated, pre-IPA, conventions; regional orthographic conventions; and individual linguist inventions. These resulted in highly ambiguous representations that could easily lead to incorrect interpretations of the data: characters associated with a given sound used to represent an entirely different sound (*h* used for glottal stop; *y* used for *θʸ*); one character used to represent various sound segments (*g* used for velar nasal, voiced velar stop, and voiced uvular stop); one set of characters used to represent multiple series of sound segments (two adjacent vowels used for adjacent like vowels, vowel-glottal stop-vowel, and long vowels); double characters used to indicate one segment (*ng* used for velar nasal; *nC* used for prenasalised consonants; *hC* or *Ch* used for aspirated consonants); diacritical marking on vowels used to indicate length (*v̄* used for long vowels); and diacritical marking on or between vowels to indicate glottal stops (*v̀* used for glottal stop-vowel; *v'v* used for vowel-glottal stop-vowel). Linguists applying methods for automatic cognate detection should always keep in mind that our data is usually rather sparse and therefore not robust enough to cope with these problems alone. As a result from our experience in working with the Polynesian data sample, we therefore advise all users of LingPy to make sure that their data is properly segmented. In the following we will provide some basic information, how this can be done.\n",
    "\n",
    "In addition to the \"normal\" requirement of the data to be written in IPA, LingPy can use the explicit segmentation of the data into sound segments as it was provided by the user. Segmentation is represented as a space-separated string in the input data, as you can see when looking at the table above, right in the cells of the `TOKENS` column. While segmentation looks unspectacular in these cases where each sound is represented by only one symbol, it may become problematic when dealing with affricates, pre-aspirated sounds, and complex vowels. The problem is usually that IPA transcriptions are inherently ambiguous, and this ambiguity is then passed on to the algorithms which cannot handle it. For example, a word like German `[`apfəl`]` \"apple\" could be either segmented as `[` a p f ə l `]`, or, and historically more consistently, as `[` a pf ə l `]`. But if the latter reading is intended (and this is usually language-family-specific), the only way to handle this consistently in IPA would be to put the bar over it: `[`ap͡fəl`]`. This practice, however, would still render the detection of pre-aspiration and other cases impossible. Although LingPy deals rather well with explicit IPA, we recommend all users to segment the data themselves and indicate this by placing one column in their input word list, in which the phonetic entries are explicitly segmented by a space (with the underscore being used to mark original spaces, i.e., word breaks).\n",
    "\n",
    "LingPy's [`sequence`-package](http://lingpy.org/reference/lingpy.sequence.html) offers many functions to handle phonetic sequences and to segment them automatically. As an example, consider the following code-pieces and try to find out what they are actually doing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 分隔语音单元\n",
    "\n",
    "在 lingpy 中，tokens 是用于处理和分析语言数据的基本单元。它们在语言学数据处理和比较中具有重要作用。以下是 tokens 的主要作用：\n",
    "\n",
    "1. 文本分割：tokens 用于将文本分割成更小的单元，可以更方便地进行统计分析和处理。\n",
    "\n",
    "2. 比较分析：在历史语言学和比较语言学中，tokens 被用来比较不同语言或方言的词汇和音节。这有助于识别语言之间的相似性和差异，从而帮助研究语言的演变和关系。\n",
    "\n",
    "3. 计算相似度：lingpy 提供了多种计算语言单元（如词或音节）相似度的算法。通过使用 tokens，可以计算出不同词汇或音节之间的相似度分数，这对于构建*语言树*或进行*聚类分析*非常有用。\n",
    "\n",
    "4. 对齐和分析：在语言对齐任务中，tokens 被用于对齐不同语言或方言的对应词汇或音节。这对于研究音变规律和语言演变模式非常重要。\n",
    "\n",
    "5. 数据规范化：tokens 也用于数据的规范化处理，例如去除标点符号、转换大小写等，以确保数据的一致性和可比性。\n",
    "\n",
    "例如，在使用 lingpy 进行词汇比较时，可以将单词分割成音节 tokens，然后使用这些 tokens 进行音节级别的比较和对齐分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.sequence.sound_classes import ipa2tokens\n",
    "\n",
    "seq1, seq2, seq3, seq4, seq5, seq6 = (\n",
    "    \"th o x t a\", \"thoxta\", \"apfəl\", \"tʰoxtɐ\", \"dɔːtər\", \"tsuŋ\")\n",
    "\n",
    "#print(seq1, \"\\t->\\t\", '\\t'.join(ipa2tokens(seq1)))\n",
    "print(seq2, \"  \\t->\\t\", '\\t'.join(ipa2tokens(seq2, semi_diacritics=\"h\")))\n",
    "print(seq3, \"  \\t->\\t\", '\\t'.join(ipa2tokens(seq3)))\n",
    "print(seq3, \"  \\t->\\t\", '\\t'.join(ipa2tokens(seq3, semi_diacritics=\"f\")))\n",
    "print(seq4, \"  \\t->\\t\", '\\t'.join(ipa2tokens(seq4)))\n",
    "print(seq5, \"  \\t->\\t\", '\\t'.join(ipa2tokens(seq5)))\n",
    "print(seq6, \"  \\t\\t->\\t\", '\\t'.join(ipa2tokens(seq6, semi_diacritics=\"s\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from these examples that LingPy's `ipa2tokens` function automatically identifies diacritics and the like, but that you can also tweak it to some extent. If the sequence contains white spaces, as in the first example, `ipa2tokens` will split by white space and assume that the data is *already* segmented. We won't go into the details of this and other functions here, but you should consider giving the documentation a proper read before you start spending time on segmenting your data manually. At the same time, when trusting LingPy's default algorithm for segmentation, you should always make sure after using it that the segmentations make sense. If they are largely wrong or problematic, you should refine them before running any automatic cognate detection method.\n",
    "\n",
    "An alternative is to use the [`segments`](https://github.com/cldf/segments) package by Moran and Forkel (2017), whose main idea is more comprehensively described in [Moran and Cysouw (2017)](http://bibliography.lingpy.org?key=Moran2017). We have in fact been using it for our working example in order to segment the ABVD data on Polynesian languages properly and afterwards checked them manually. Our orthography profile for the Polynesian languages looks like this (we only show the first couple of lines):\n",
    "\n",
    "Grapheme\t| IPA |\tNOTE\n",
    "--- | --- | ---\n",
    "q\t| ʔ\t| \n",
    "ŋg |\tⁿg\t| account for pre-nasalization\n",
    "0\t| NULL\t|\n",
    "ng | ŋ\t|\n",
    "t |\tt\t|\n",
    "\n",
    "So what you can essentially see here is that we have an initial column in which we list the graphemes that we find in the data, and we add a second column that says how we would like to render it in IPA. If we write \"NULL\", this means we want to discard it. With the [segments](https://github.com/bambooforest/segments) package, this can be applied to our data in a very straightforward way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v a ' u\n",
      "v a ʔ u\n",
      "v ã ĩ õ ɔ̃ ũ\n"
     ]
    }
   ],
   "source": [
    "from segments.tokenizer import Tokenizer\n",
    "from lingpy.basic.wordlist import Wordlist\n",
    "tk = Tokenizer(r'C:\\Users\\sheng\\OneDrive\\文档\\论文\\LingPy\\lingpy-tutorial\\orthography.tsv')\n",
    "print(tk(\"va'u\"))\n",
    "print(tk(\"va'u\", column=\"IPA\"))\n",
    "print(tk(\"vãĩõɔ̃ũ\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成正字法文件\n",
    "下面这一行命令从方言材料中自动提取国际音标符号列表，并保存为profile，用于拆分国际音标为Tokens.Tokens用于语音对齐、计算同源词距离等。\n",
    "\n",
    "$ lingpy profile -i data/P_profile-creation.tsv -o data/P_created-profile.tsv --column=ipa\n",
    "\n",
    "你也可以将输入文件名P_profile-creation.tsv改成你的方言数据表文件的名称，生成的文件正字法文件名称P_created-profile.tsv改为任意其他的名称，如下面的代码中使用P_wenzhou-orthography.tsv。\n",
    "\n",
    "有了这个正字法文件，就可以给每个音节进行拆分，Tokenizer的作用是给国际音标之间加上一个空格变成Tokens，用于音素对齐，转换成音类，然后进行分析比较，计算音节之间的相似度和距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 19:31:37,875 [INFO] Data has been written to file <HEAD_COGNATES-segments2.tsv>.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from lingpy import *\n",
    "from segments.tokenizer import Tokenizer\n",
    "from lingpy.basic.wordlist import Wordlist\n",
    "tk = Tokenizer(r'C:\\Users\\sheng\\OneDrive\\文档\\论文\\LingPy\\lingpy-tutorial\\P_wenzhou-orthography.tsv')\n",
    "wl2 = Wordlist('head_Cognates.tsv')\n",
    "wl2.add_entries('new_segments', 'FORM', tk, column=\"IPA\")\n",
    "wl2.output('tsv', filename='HEAD_COGNATES-segments2', ignore='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the data has been segmented and converted to IPA at the same time, which is very convenient, if the original data is not very well-represented in form of phonetic transcriptions. If you want to convert all segments in your data at once, thus modifying your Wordlist, you can do this in a very convenient way, using the ```wordlist.add_entries``` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wl.add_entries('new_segments', 'form', tk, column=\"IPA\")\n",
    "wl.output('tsv', filename='polynesian-new-segments', ignore='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check how well the data has been converted, you could either open the output file ```polynesian-new-segments.tsv``` in a text editor or your favorite spreadsheet editor. You could also open the EDICTOR tool in your browser at [http://edictor.digling.org](http://edictor.digling.org) and drag the file into the BROWSE window, where you have many enhanced options on comparing and checking the data (for a recent EDICTOR tutorial, see [List 2017](http://bibliography.lingpy.org?key=List2017LECTUREc)).\n",
    "\n",
    "If you want to check, how well the data has been converted so that LingPy can link the sounds to its internal representation, you can also do this with help of the ```check_tokens``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in position 2: «'»\n"
     ]
    }
   ],
   "source": [
    "from lingpy.sequence.sound_classes import check_tokens\n",
    "tokens = \"v a ' h u\".split(' ')\n",
    "errors = check_tokens(tokens)\n",
    "for error in errors:\n",
    "    print('Error in position {0[0]}: «{0[1]}»'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do this for all entries in your list, just iterate over all tokens, and store the errors in a dictionary, where you can also count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "errors = defaultdict(int)\n",
    "for idx, tks in wl.iter_rows('tokens'):\n",
    "    for error in check_tokens(tks):\n",
    "        errors[error[1]] += 1\n",
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we do not have any errors in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Checking Coverage\n",
    "\n",
    "For cognate detection, it is not only important to have good phonetic transcriptions (ideally segmented in such a form that they were checked by an experienced linguist), but also to make sure that there are **enough words** in your data. If the data is too sparse, even human linguists would not be able to find any signal based on regular sound correspondences, provided they see the languages the first time and don't know their history (which is the situation for every algorithm). Following an earlier study by [List (2014b)](http://bibliography.lingpy.org?key=List2014c), we know now that at least 100 word pairs for languages as disparate as English and French are needed to provide a solid basis for automatic cognate detection. But when dealing with a large dataset of different languages, which necessarily contains a number of gaps (not all concepts can be elicited in the sources, field work has not provided enough details, etc.), it can be deleterious if the *mutual coverage* between the languages is low. \n",
    "\n",
    "By mutual coverage, I mean the number of comparable word pairs (with the same concept) for each language pair in a given dataset. We can compare different aspects of mutual coverage, such as the *average mutual coverage平均互相覆盖率*, where we average the number of available word pairs, or the *minimal mutual coverage*, which provides the smallest mutual coverage of any pair of languages. In addition, one can also ask for the subset fulfilling a minimal mutual coverage for all language pairs, and this task would return the subset of languages in a `Wordlist` which all have at least the mutual coverage specified by the user. LingPy offers now (since version 2.5.1, see also the [online reference](http://lingpy.org/docu/compare/sanity.html)) solutions for all these problems, but since the last problem is considerably hard and computationally intensive, we won't discuss it here, but will instead simply check the minimal mutual coverage which holds for all languages in our sample. So we try to find the lower bound of concept pairs which all languages have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 检查平均互相覆盖率\n",
    "\n",
    "假设我们有一个包含100个概念的词表，并且有两种语言（L1和L2），它们的词汇覆盖情况如下：\n",
    "\n",
    "L1：有50个概念有对应的词，剩下50个概念没有对应的词。\n",
    "L2：有70个概念有对应的词，剩下30个概念没有对应的词。\n",
    "在数据库中，这意味着对于L1和L2的词表，有部分行是空的，因为某些概念在某种语言中没有对应的词。\n",
    "而且L1的50个对应说法的概念跟L2的70个概念之间还只是部分重叠，比如两种语言都有说法的概念只有30个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.compare.util import (\n",
    "    mutual_coverage_check, mutual_coverage_subset)\n",
    "for i in range(210, 0, -1):\n",
    "    if mutual_coverage_check(wl, i):\n",
    "        print(\n",
    "            \"Minimal mutual coverage is at {0} concept pairs.\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is definitely good enough for our purpose, given the rule of thumb which says that below a minimal mutual coverage of 100 one should not do language-specific cognate detection analyses. If the coverage is lower, this does not mean you need to give up automatic cognate detection, but it means you should not use the language-specific `LexStat` method but rather a language-independent method, which does not require the information on potential sound correspondences (but will also tend to identify more false positives).\n",
    "\n",
    "Another score that is useful to compute when carrying out these analyses is what is called *Average Mutual Coverage* (AMC) in LingPy. The AMC of a given wordlist is defined as the average of the number of concepts shared between all pairs of languages in a given wordlist divided by the number of concepts in total. The computation in LingPy is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.compare.sanity import average_coverage\n",
    "print('{0:.2f}'.format(average_coverage(wl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is in fact quite high, since we know that many other datasets that have been used in the past to test automatic cognate detection methods had a considerable amount of missing data and would therefore have an AMC score much lower than 0.8.\n",
    "\n",
    "Although, as we just said, the value is good enough, we should further reduce the data a bit to make sure we can inspect them better later on (otherwise, the analyses may also take a lot of time if you run them on computers with insufficient power). So what we will do right now is testing the `mutual_coverage_subset` method which returns a subset of languages for which a given minimal mutual coverage holds. We will then export our `Wordlist` object to file by specifying these languages as our subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, results = mutual_coverage_subset(wl, 200)\n",
    "coverage, languages = results[0]\n",
    "print(\n",
    "    'Found {0} languages with an average mutual coverage of {1}.'.format(\n",
    "        count, coverage))\n",
    "\n",
    "# write word list to file\n",
    "wl.output(\"tsv\", filename=\"polynesian-small\",\n",
    "          subset=True, rows=dict(doculect = \"in \"+str(languages)))\n",
    "\n",
    "# load the smaller word list\n",
    "wl = Wordlist('polynesian-small.tsv')\n",
    "\n",
    "# print basic characteristics\n",
    "print(\n",
    "    \"Wordlist has {0} languages and {1} concepts in {2} words.\".format(\n",
    "        wl.width, wl.height, len(wl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not further work with this selection of languages with a very high coverage, and it is always recommended to do so when working on diverse languages samples. For our further tests, however, we will restrict our selection of languages to another subset, namely the East Polynesian languages. Let us now extract those languages from the data (based on their language names) and then see how good the coverage is for this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastern = ['North_Marquesan', 'Ra’ivavae', 'Rurutuan', \n",
    "            'Tahitian', 'Sikaiana', 'Maori', 'Hawaiian',\n",
    "            'Mangareva', 'Tuamotuan', 'Rapanui'] \n",
    "wl = Wordlist('polynesian.tsv')\n",
    "wl.output('tsv', filename='east-polynesian', subset=True,\n",
    "            rows=dict(doculect = 'in '+str(eastern)))\n",
    "\n",
    "wl = Wordlist('east-polynesian.tsv')\n",
    "print(\n",
    "    \"East Polynesian data has {0} concepts and {1} languages.\".format(\n",
    "        wl.height, wl.width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now repeat the coverage experiment from above, but this time with the Eastern Polynesian language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(210, 0, -1):\n",
    "    if mutual_coverage_check(wl, i):\n",
    "        print(\"Minimal mutual coverage is at {0} concept pairs (AMC: {1:.2f}).\".format(\n",
    "            i, average_coverage(wl)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this coverage is much less than the coverage we encountered above. Nevertheless, for our purpose it will be good enough, and the rule of thumb for closely related languages, which says, that we need more than 150 concepts mutually shared between each language pair holds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Checking for Synonyms\n",
    "\n",
    "Extreme number of synonyms per concepts may drastically confuse the algorithms, so we should check and make sure that we do not have too many of them in our data. This can be conveniently done in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.compare.sanity import synonymy\n",
    "synonyms = synonymy(wl)\n",
    "for (language, concept), count in sorted(\n",
    "    synonyms.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 3:\n",
    "        print('{0:15}  {1:12}  {2}'.format(language, concept, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have some cases which we should be careful about. There are many ways to fall, but six words for \"to fall\" in North Marquesan are surely not satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Phonetic Alignment\n",
    "\n",
    "In the main tutorial text, we explain the major aspects of alignment analyses. We won't repeat them here and recommend you to read them carefully in the main document instead. What we want to do here is quickly run you through the major aspects of alignments in LingPy. We will start with pairwise alignments and then introduce multiple alignments. Since multiple alignments are based on pairwise alignments, the major aspects, such as the scoring function, the gap function, and the representation of sound classes, are something you should always keep in mind, especially when you find that automatic alignments do not conform to your expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pairwise Alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Scoring Function\n",
    "\n",
    "The scoring function in pairwise alignments determines how segments are compared with each other. If the score is high, this means that the sounds are judged to be likely to be related, if it is below 0 (that is, if it has a negative score) this means that it is assumed that a relationship is unlikely. Scoring functions in LingPy are usually stored in so-called sound-class models. We will introduce sound classes in detail in the next section, so here it should be enough to mention that our scorers are not defined for pure IPA segments, but rather for subsets,  which make it more convenient to define scores for dissimilarities. Instead of comparing `[`tʰ`]` with `[`tʃ`]`, we internally compare ```T``` with ```C```, which is faster to compute, but also easier to define. In the following, we will work with the SCA-scorer, a scorer for the scoring function underlying the SCA algorithm ([List 2014](http://bibliography.lingpy.org?key=List2014d)). We can easily load the scorer in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 评分函数\n",
    "\n",
    "在成对比对（pairwise alignments）中，评分函数（scoring function）决定了音段之间的比较方式。如果得分很高，这意味着这些音段被认为很可能是相关的；如果得分低于0（即负分），这意味着假设它们之间的关系不太可能。LingPy 中的评分函数通常存储在所谓的音类模型（sound-class models）中。我们将在下一节详细介绍音类。在这里，只需提到我们的评分器（scorers）不是为纯粹的国际音标（IPA）音段定义的，而是为子集定义的，这使得定义不相似的得分更为方便。与其将[tʰ]与[tʃ]进行比较，我们在内部将T与C进行比较，这样计算更快，而且定义起来也更容易。接下来，我们将使用SCA评分器，这是用于SCA算法（List 2014）的评分函数。我们可以通过以下方式轻松加载评分器：\n",
    "\n",
    "Model 类：Model 是 LingPy 中的一个类，用于创建音位比较模型。模型包含了用于计算两个音位之间相似度的矩阵，称为评分矩阵（scoring matrix）。\n",
    "SCA 模型：sca 是 Sound-Class-Based Phonetic Alignment 的缩写，是一种基于音类的音韵对齐模型。SCA 模型根据语言音段的分类（如辅音、元音等）计算它们之间的相似度，用于对词汇进行音韵对齐和比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Model('sca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on the underlying [Model](http://lingpy.org/docu/data/model.html) class can be found in LingPy's online reference. In order to test, how a given sound class compares to another one, we can conveniently call the scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symA in ['P', 'T', 'A', 'E']:\n",
    "    for symB in ['T', 'C', 'I']:\n",
    "        print(symA, symB, scorer(symA, symB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that, as a rule, vowels (```AEI```) and consonants (```PTC```) always get -10 as score. If consonants are not judged to be similar, they get a zero, but if we assume that there is a certain likelihood that sound change will create sound correspondences across classes, the scorer gives more detailed scores.\n",
    "\n",
    "你可以看到，通常元音（AEI）和辅音（PTC）总是得到-10的分数。如果辅音不被认为是相似的，它们的得分是零，但如果我们假设声音变化在不同类别之间会产生一定的对应关系，那么评分器会给出更详细的分数。\n",
    "\n",
    "## 解释\n",
    "这段话讨论了一个评分系统的规则，具体如下：\n",
    "\n",
    "*元音和辅音的基础得分：*\n",
    "1. 元音（如A、E、I）和辅音（如P、T、C）在默认情况下都会得到一个基础分数-10。这说明它们在某种标准下被评定为相似或相关的基础分数。\n",
    "2. 辅音相似度判断：如果辅音被判断为不相似，它们的得分是零。这意味着辅音之间没有相关性或相似性。\n",
    "3. 声音变化的假设：如果假设声音变化会在不同类别（如元音和辅音之间）产生一定的对应关系，评分系统会给出更加详细和复杂的分数。这说明在某些情况下，评分器会考虑声音变化带来的影响，可能是基于语言学中的音变规律。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Sound Classes \n",
    "\n",
    "Above, we wrote that LingPy takes care of the *word form* as one of the units of the linguistic sign in the classical \"Saussurean\" model. But how can we know whether LingPy recognizes a symbol or not? For this, we need to understand what LingPy does internally with word forms. Here, LingPy follows [Dolgopolsky's (1964)](http://bibliography.lingpy.org?key=Dolgopolsky1964) idea of \"sound classes\", namely the idea that we can break down the complexity inherent in phonetic transcription to some major classes of sounds so that those sounds represent some kind of a coherent unit. Dolgopolsky was thinking of sounds which often occur in correspondence relation to each other, assuming that there is a certain sound-correspondence probability inherent in all sounds (see also [Brown, Holman, and Wichmann 2013](http://bibliography.lingpy.org?key=Brown2013)). In our experience so far, this is definitely one important aspect, but even more important is the role of reducing variation which is unnecessary for historical comparison while at the same time maintaining a sufficient degree of distinctiveness. For this reason, I expanded Dolgopolsky's original system of only 10 sound classes to as many as 25 sound classes, and LingPy further offers the alphabet which was used for the [ASJP project](http://asjp.clld.org), which consists of 40 symbols in a slightly modified version. The following image illustrates the differences between these sound class alphabets and also shows how they represent the Greek word for \"daughter\".\n",
    "## 音类\n",
    "上文提到，我们写到LingPy将词形视为经典“索绪尔”模型中语言符号的一个单位。但是我们怎么知道LingPy是否识别某个符号呢？为此，我们需要了解LingPy在内部如何处理词形。在这方面，LingPy遵循Dolgopolsky（1964年）的“音类”概念，即将语音转写中的复杂性分解为几类主要的声音，使这些声音代表某种连贯的单位。Dolgopolsky认为，有些声音经常互相对应，并假设所有声音中存在某种固有的声音对应概率（参见Brown、Holman和Wichmann 2013年）。根据我们的经验，这无疑是一个重要方面，但更重要的是减少在历史比较中不必要的变异，同时保持足够的区分度。基于这个原因，<span style=\"color: blue; font-weight: bold;\">我将Dolgopolsky最初只有10个音类的系统扩展到了多达25个音类，LingPy还提供了ASJP项目所使用的字母表，该字母表在稍作修改后包含40个符号。</span>下图说明了这些音类字母表之间的差异，并展示了它们如何表示希腊语中的“女儿”一词。\n",
    "\n",
    "![image](img/soundclasses.jpg)\n",
    "\n",
    "虚线框和不同的背景颜色代表了不同的音类系统中音段的分类方式。具体来说，图片展示了三个音类系统（Dolgopolsky、SCA和ASJP）中音段的分类方式。每一类音段在该系统中被视为相似或等价，比如[z, s, ʃ, ʒ] 被归为同一类。这些音段在Dolgopolsky系统中被视为相似。\n",
    "\n",
    "How can we represent sound classes in LingPy? There is one main function that converts a segmented sound sequence into sound classes. This function `tokens2class` takes as input a list or a tuple of segments, that is, the output which you would also get when calling `ipa2tokens`, and a valid sound class model. You can theoretically create models yourself, and pass them as an instance of the above-mentioned `Model` class in LingPy, but for the moment, we will only use the ones which are there and denote them with strings, i.e., `dolgo` for Dolgopolsky's model, `sca` for my expanded model of Dolgopolsky, and `asjp` for the ASJP model). Let's just take these three and another specific model, called `art` (for \"articulation\") which gives numbers to indicate the prosody of sounds, and convert the word Greek `[`θiɣatɛra`]` into the different sound class systems.\n",
    "\n",
    "如何在 LingPy 中表示音类？有一个主要函数可以将分割的音序列转换为音类。这个函数 tokens2class 接受一个由音段组成的列表或元组作为输入，也就是调用 ipa2tokens 时得到的输出，以及一个有效的音类模型。理论上，你可以自己创建模型，并将其作为上述 LingPy 中 Model 类的一个实例传递，但目前我们只使用现有的模型，并用字符串表示它们，即 Dolgopolsky 模型的 dolgo，我扩展的 Dolgopolsky 模型的 sca，以及 ASJP 模型的 asjp。我们就用这三种模型和另一种叫做 art（表示“发音”）的特定模型，该模型用数字来表示声音的音韵，并将希腊语单词 θiɣatɛra 转换为不同的音类系统。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"θiɣatɛra\"\n",
    "segs = ipa2tokens(word)\n",
    "\n",
    "# iterate over sound class models and write them in converted version \n",
    "for model in ['dolgo', 'sca', 'asjp', 'art']:\n",
    "    print(word, ' -> ', ''.join(tokens2class(segs, model)), '({0})'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Gap Function\n",
    "\n",
    "So far, we have not even aligned a sequence pair, but introduced sound classes and the scoring function. Let us now, in order to illustrate the gap function, and why it may be important, make our first pairwise alignment in LingPy. The gap penalty is the so-called gap-open-penalty缺口开启惩罚, and it is triggered by changing the ```gop``` keyword when aligning two words in the ```Pairwise``` class (see also the [online referen](http://lingpy.org/docu/align/pairwise.html)):\n",
    "\n",
    "参数 gop=-0.5 的意义\n",
    "Gap Opening Penalty (GOP):\n",
    "1. 当在一个序列中引入一个缺口时（例如，为了更好地对齐两个序列中的相似部分），算法会施加一个惩罚。\n",
    "2. gop 参数的值越低（越负），缺口引入的惩罚就越大。这意味着算法会更倾向于避免引入缺口，除非这是唯一的对齐方式。\n",
    "3. gop=-0.5 表示每次引入一个缺口的惩罚分数为 -0.5。这个值是一个负数，表示引入缺口会减少整体比对的得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = Pairwise('ʔ oː ɢ u a', 'k oː r u a')\n",
    "pair.align(gop=-1)\n",
    "print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences align as expected (the score in the last row indicates the overall similarity), but we know that the default scorer of LingPy does not approve of the similarity between `[`ɢ`]` and `[`r`]`. Instead it would favor linking the former with `[`k`]`. If we trigger the gap opening penalty, this thus may change the alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = Pairwise('ʔ oː ɢ u a', 'k oː r u a')\n",
    "pair.align(gop=-0.5)\n",
    "print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Alignment mode\n",
    "\n",
    "In the main text introducing the tutorial, we give examples in Figure 3D for different output produced from the alignment mode. Let us replicate this here (note that \"overlap\" is the keyword you need to use if you want to make \"semi-global\" alignments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = Pairwise('ʔ oː ɢ u a', 'k oː r u a')\n",
    "for mode in ['global', 'local', 'overlap']:\n",
    "    pair.align(mode=mode)\n",
    "    print(pair)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multiple Alignment\n",
    "\n",
    "Phonetic alignment is *per se* independent of the existence of any word list data. Instead, it is a way to align phonetic sequences (words in phonetic transcription) in various ways. Phonetic alignment is an important pre-requisite in order to identify regular sound correspondences. Regular sound correspondences again are important to identify cognates (at least in the classical framework of the comparative method). In addition, alignment analyses are useful in presenting one's analyses in a transparent way, since, unfortunately, scholar often think that their cognate judgments are self-evident, ignoring that a linguist with another language family as their specialty will barely be able to follow the idiosyncratic discourse on language-family-specific sound change patterns and the like. \n",
    "\n",
    "In order to carry out alignment analyses in LingPy, you have a range of different possibilities, and there won't be the time to cover all of them here. Instead, I will illustrate how you can make a quick multiple alignment using the ```Multiple``` class of LingPy (see also the [online reference](http://lingpy.org/docu/align/multiple.html)). This class is automatically imported when importing LingPy, and it requires a list of sequences as input. Here again, LingPy will automatically try to split your input sequences if they are not already segmentized, but we advise you to segmentize them properly before. We use four words for \"dog\" in Polynesian languages (Samoan, Hawaiian, North Marquesan, and Anuta). We do not type them in by pre-segmenting them, but rather tell LingPy to treat vowels not as dipthongs. We start with the simplest method, the *progressive alignment*, which first makes a little tree of the input sequences and then aligns them by going the tree from the leaves to the root, every time aligning two more until all are aligned:\n",
    "### 3.2多重语音比对\n",
    "语音比对本质上独立于任何词表数据的存在。它是一种以各种方式比对语音序列（语音转写中的单词）的方法。语音比对是识别规律性音对应的一个重要前提。而规律性音对应对于识别同源词（至少在比较方法的经典框架中）至关重要。此外，比对分析在以透明的方式展示分析结果方面也非常有用，因为不幸的是，学者们往往认为他们的同源词判断是显而易见的，忽略了一个专攻其他语言家族的语言学家几乎无法理解关于特定语言家族的音变模式等的独特论述。\n",
    "\n",
    "为了在 LingPy 中进行比对分析，你有一系列不同的可能性，这里没有时间涵盖所有这些方法。相反，我将展示如何使用 LingPy 的 Multiple 类进行快速多重比对（参见在线参考资料）。导入 LingPy 时，该类会自动导入，并且它需要一个序列列表作为输入。同样，如果输入序列尚未分段，LingPy 会自动尝试拆分你的输入序列，但我们建议你在此之前正确分段。我们使用波利尼西亚语言中的四个“狗”这个词（萨摩亚语、夏威夷语、北马克萨斯语和阿努塔语）。我们不通过预先分段来输入这些词，而是告诉 LingPy 不将元音视为双元音。我们从最简单的方法，即*渐进比对开始，该方法首先生成输入序列的一个小树，然后通过从树叶到树根进行比对，每次比对两个序列，直到所有序列都被比对完毕*："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = Multiple(['ʔuli', 'ʔilio', 'kuʔi', 'kori'], merge_vowels=False)\n",
    "print(msa.align('progressive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more complicated algorithms available, for example, library-based alignment, following the T-Coffee algorithm ([Notredame et al. 2000](http://bibliography.lingpy.org?key=Notredame2000)), based on a so-called \"library\" which is created before the tree is built. \n",
    "\n",
    "还有更复杂的算法可用，例如基于库的比对，遵循 T-Coffee 算法（Notredame 等，2000），该算法基于在生成树之前创建的所谓“库”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msa.align('library'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still the same, which is not really surprising, given that this alignment is not very challenging, but it was shown in [List (2014)](http://bibliography.lingpy.org?key=List2014d) that this algorithm largely enhances more complex alignments.\n",
    "\n",
    "As mentioned before, the algorithms make use of a specific *guide tree* along with the sequences are consecutively aligned. In order to check how this guide tree looks like, you can do the following:\n",
    "\n",
    "这段话提到的算法使用了一个特定的指导树，与序列一起连续对齐。为了检查这个指导树的样子，你可以执行以下操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msa.tree.asciiArt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm guide tree shows the sound-class reresentation of the words as the leaves of the tree. From there, it is probably also quite easy to see how the algorithm arrives at the cluster decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Cognate Detection\n",
    "\n",
    "### 4.1 Checking the Data\n",
    "\n",
    "We assume that you have thoroughly checked your data manually before running cognate detection analyses. I also assume that you do not have any of the following problems in your data:\n",
    "\n",
    "* an extensive number of synonyms in one language\n",
    "* multiple variant forms for the same word form\n",
    "* data merged from different sources without adjusting the phonetic transcription\n",
    "* mutual coverage below 100 words per language pair (and AMC score below 0.75).\n",
    "\n",
    "Before running the cognate detection analysis, you may, however, still want to check whether LingPy recognizes all your data correctly. Here, a very simple way to achieve this is to load the `LexStat` class with the specific keyword `check` set to `True` (more on LexStat can also be found in the [online reference](http://lingpy.org/docu/compare/lexstat.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = LexStat('east-polynesian.tsv', check=True, segments='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexwen = LexStat('P_wenzhou-alignment-file.tsv', check=True, segments='tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "If you have problems in your data encoding, you will be asked if you want to exclude the sequences automatically. As a result, a logfile, called `errors.log` will be created and point you to all erroneous sequences which contain segments which LingPy does not recognize. Let us quickly introduce some bad sequences by just converting randomly all `[`e`]` sounds to the letter A (capitals are never accepted in the normal sound class models of LingPy) and see what we get then. For this, we even do not need to re-write the data, we just add another row where we change the content, give it a random name (we call it \"tokens\", as this also signals LingPy that the input should be treated as a sequence and not as a string), and specify this for the `LexStat` instance method as the column in the file where the `segments` are. We first load the data as `Wordlist` and then pass that data directly to `LexStat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:05:20,205 [INFO] Data has been written to file <errors.log>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were errors in the input data - exclude them? [y/N]  N\n"
     ]
    }
   ],
   "source": [
    "wl = Wordlist('east-polynesian.tsv')\n",
    "\n",
    "# add new column \"segments\" and replace data from column \"tokens\"\n",
    "wl.add_entries('segments', 'tokens', lambda x: ['A' if y == 'e' else y for y in x])\n",
    "\n",
    "lex = LexStat(wl, segments='segments', check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now check the file `errors.log`, you will find a long file with the following first ten lines:\n",
    "\n",
    "```text\n",
    "ID\tTokens\tError-Type\n",
    "1572\t<bad character in tokens at «A»>\tg a t o a A l i m a\n",
    "3320\t<bad character in tokens at «A»>\tp a A _ ʔ a ʔ u r u\n",
    "5145\t<bad character in tokens at «A»>\tl i m a s A f u l u\n",
    "5696\t<bad character in tokens at «A»>\tr i m a _ t A k a u\n",
    "12\t<bad character in tokens at «A»>\tp a A\n",
    "3327\t<bad character in tokens at «A»>\tp a A\n",
    "5153\t<bad character in tokens at «A»>\tA _ f aː\n",
    "```\n",
    "\n",
    "Each row starts with the ID of the word which is contaminated (and this links to the row-ID of your input file), it is followed by a description of the error-type, and then by a segmented form of the word form. LingPy then also creates a file called `lingpy-DATE_cleaned.tsv` (`DATE` meaning the date of the day you run LingPy), in which all contaminated words have been excluded, and this file is read in again, if you pressed \"y\", and will be the one to run the analysis. \n",
    "\n",
    "LingPy thus tries to make the enterprise of cognate detection quite convenient for you as a user, but you should be warned not to use files containing errors for publications, but only for personal test purposes, in order to improve your data. If LingPy does not recognize characters, you should not globally exclude them as a reaction, but should instead try to improve your data until it is publication-ready. Otherwise, the results will much likely be disappointing anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Overview on Algorithms\n",
    "\n",
    "LingPy comes along with four pre-defined cognate detection algorithms. These algorithms are all contained in the `LexStat` class which often confuses users, as one of the algorithms provided by `LexStat` is also called `lexstat`. Internally, however, it makes sense, as all algorithms were created at the same time, when the LexStat algorithm was published ([List 2012b](http://bibliography.lingpy.org?key=List2012a)), so it was written into one class which was called \"LexStat\".\n",
    "\n",
    "When carrying out cognate detection algorithms, it is important to keep in mind what these algorithms are based on. We can distinguish the following three major types:\n",
    "\n",
    "1. consonant-class-matching (CCM), following Dolgopolky's (1964) early idea to assume that words with two matching consonant classes would likely be cognate,\n",
    "2. phenotypic sequence similarity partitioning (PSSP), follows the general idea also applied in homology detection in biology, by which sequences are clustered into sets of homologs based on a partitioning algorithm which is applied to a distance or a similarity matrix representing the overall sequence similarity,\n",
    "3. language-specific sequence similarity partitioning (LSSP), follows the core idea of the LexStat algorithm by which sequence similarity is calculated on a language-specific basis for each language pair in the data, based on permutation statistics which give hints regarding the most likely candidates for regular sound correspondences.\n",
    "### 同源检测算法\n",
    "在进行同源词检测算法时，重要的是要了解这些算法的基础。我们可以区分以下三种主要类型：\n",
    "\n",
    "1. 辅音类匹配 (CCM)：遵循Dolgopolksy（1964）的早期观点，假设如果两个词在辅音类上匹配，它们可能是同源词。\n",
    "2. 表型序列相似性划分 (PSSP)：类似于生物学中的同源性检测（Homology Detection）方法，将序列聚类为同源集合（Sets of Homologs)，并通过计算序列之间的距离或相似性来聚类。\n",
    "3. 语言特定序列相似性划分 (LSSP)：基于LexStat算法，通过排列统计（permutation statistics）为每对语言计算特定的相似性。\n",
    "\n",
    "In LingPy, the methods which you can use to carry out these analyses have specific names, as well as the default output, a cluster decision represented as an integer identifier that assigns words to clusters. They are given in the table below:\n",
    "\n",
    "Class | Alignments? | Sound Classes? | Sound Correspondences? | Threshold?| LingPy-Name | LingPy-Output | Note\n",
    "--- | --- | --- | --- | --- | ---\n",
    "CCM | no | yes | no | no | \"turchin\" | \"turchinid\" | Consonant-class matching method close to the description in [Turchin et al. (2010)](http://bibliography.lingpy.org?key=Turchin2010))\n",
    "PSSP | yes | no | no | yes | \"edit-dist\" | \"editid\" | Vanilla edit-distance ([Levenshtein 1965](http://bibliography.lingpy.org?key=Levenshtein1965)), normalized by dividing with the longer string.\n",
    "PSSP | yes | yes | no | yes | \"sca\" | \"scaid\" | Distance score derived from SCA alignments ([List 2012a](http://bibliography.lingpy.org?key=List2012b)) by applying [Downey et al.'s (2008)](http://bibliography.lingpy.org?key=Downey2008) formula\n",
    "LSSP | yes | yes | yes | yes | \"lexstat\" | \"lexstatid\" | The core \"LexStat\" algorithm described in List ([2012b](http://bibliography.lingpy.org?key=List2012a) and [2014](http://bibliography.lingpy.org?key=List2014d))\n",
    "\n",
    "As a general rule, you should keep the following in mind (see also our experience with these methods in [List, Greenhill, and Gray (2017)](http://bibliography.lingpy.org?key=List2017c):\n",
    "\n",
    "1. if you want a fast first analysis and speed counts, take \"turchin\" (Dolgopolsky method), since it has a low amount of false positives, but it will also miss many cognates\n",
    "2. if speed does not matter and you have enough concepts (> 100) in your data, and you want to have the most reliable analysis, take \"lexstat\"\n",
    "3. if you have less than 100 concepts, and speed does not really matter, take \"sca\", as it yields consistently better results as the \"turchin\" method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Running the Analyses\n",
    "\n",
    "In the following, we will run the users quickly to a test of all cognate detection algorithms and learn how to compare them. We start with the method called \"turchin\" in LingPy, and referred to as *consonant-class matching method* (CCM) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = LexStat('east-polynesian.tsv', segments='tokens', check=True)\n",
    "\n",
    "# run the dolgopolsky (turchin) analysis, which is threshold-free\n",
    "lex.cluster(method='turchin', ref='turchinid')\n",
    "lex.cluster(method=\"edit-dist\", threshold=0.75, ref='editid')\n",
    "\n",
    "# show the cognate sets, stored in \"turchinid\" for the words for \"Eight\"\n",
    "eight = lex.get_dict(row='Eight') # get a dictionary with language as key for concept \"eight\"\n",
    "for k, v in eight.items():\n",
    "    idx = v[0] # index of the word, it gives us access to all data\n",
    "    print(\"{0:20} \\t {1} \\t{2}\\t {3}\".format(lex[idx, 'doculect'], lex[idx, 'value'], \n",
    "                                             lex[idx, 'turchinid'],\n",
    "                                            lex[idx, 'editid']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 导入 LexStat 模块并加载温州方言的比对文件\n",
    "lexwen = LexStat('P_wenzhou-alignment-file.tsv', segments='tokens', check=True)\n",
    "\n",
    "# 运行 Dolgopolsky (Turchin) 分析，这是一种无阈值方法\n",
    "lexwen.cluster(method='turchin', ref='turchinid')\n",
    "\n",
    "# 使用编辑距离方法进行聚类，设置阈值为0.75\n",
    "lexwen.cluster(method=\"edit-dist\", threshold=0.75, ref='editid')\n",
    "\n",
    "# 显示存储在 \"turchinid\" 中的同源词集合，这里针对的是 \"七\" 这个词\n",
    "seven = lexwen.get_dict(row='七') # 获取一个以语言为键的字典，概念为 \"七\"\n",
    "for k, v in seven.items():\n",
    "    idx = v[0] # 获取词的索引，从而访问所有数据\n",
    "    print(\"{0:20} \\t {1} \\t{2}\\t {3}\".format(lexwen[idx, 'doculect'], \n",
    "                                             lexwen[idx, 'IPA'], \n",
    "                                             lexwen[idx, 'turchinid'],\n",
    "                                             lexwen[idx, 'editid']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexwen = LexStat('P_wenzhou-alignment-file.tsv', segments='tokens', check=True)\n",
    "\n",
    "# run the dolgopolsky (turchin) analysis, which is threshold-free\n",
    "lexwen.cluster(method='turchin', ref='turchinid')\n",
    "lexwen.cluster(method=\"edit-dist\", threshold=0.75, ref='editid')\n",
    "\n",
    "# show the cognate sets, stored in \"turchinid\" for the words for \"七\"\n",
    "seven = lexwen.get_dict(row='七') # get a dictionary with language as key for concept \"七\"\n",
    "for k, v in seven.items():\n",
    "    idx = v[0] # index of the word, it gives us access to all data\n",
    "    print(\"{0:20} \\t {1} \\t{2}\\t {3}\".format(lexwen[idx, 'doculect'], lexwen[idx, 'IPA'], \n",
    "                                             lexwen[idx, 'turchinid'],\n",
    "                                            lexwen[idx, 'editid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same for the \"sca\" method, but since this method is not threshold free, we will need to define a threshold. We follow the default value we know from experience, which is 0.45. We then print out the same data, but this time including the cognate judgments by all three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.cluster(method=\"sca\", threshold=0.45, ref='scaid')\n",
    "\n",
    "for k, v in eight.items():\n",
    "    idx = v[0] \n",
    "    print(\"{0:20} \\t {1} \\t{2} \\t {3} \".format(\n",
    "        lex[idx, 'doculect'], \n",
    "        lex[idx, 'value'], \n",
    "        lex[idx, 'turchinid'], \n",
    "        lex[idx, 'scaid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to do the same analysis with the \"lexstat\" method. This will take some time due to the permutation test. In order to make sure we do not need to run this all the time, we will save the data immediately after running the permutation to a file which we give the extension \"bin.tsv\", and which we can load in case we want to carry out further tests, or which we can otherwise also share when publishing results, as it contains all the data needed to rerun the analyses on a different machine. LingPy creates a lot of data when analyzing wordlists, but by default, only a minimal amount of the data is written to file. In this case, if we want to store the results of the permutation test, we need to store the whole file with all the data that lingpy produces, especially the language-specific scoring function. In order to force LingPy to do so, we have to add the keyword ```ignore=[]``` to the output-function. This will prevent that any data which should be written to file is ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.get_scorer(runs=10000)\n",
    "lex.output('tsv', filename='east-polynesian.bin', ignore=[])\n",
    "lex.cluster(method='lexstat', threshold=0.60, ref='lexstatid')\n",
    "\n",
    "for k, v in eight.items():\n",
    "    idx = v[0] \n",
    "    print(\"{0:20} \\t {1} \\t{2} \\t {3} \\t {4}\".format(\n",
    "        lex[idx, 'doculect'], \n",
    "        lex[idx, 'value'], \n",
    "        lex[idx, 'turchinid'], \n",
    "        lex[idx, 'scaid'],\n",
    "        lex[idx, 'lexstatid']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算温州方言相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexwen.get_scorer(runs=10000)\n",
    "lexwen.output('tsv', filename='wenzhou.bin', ignore=[])\n",
    "lexwen.cluster(method='lexstat', threshold=0.60, ref='lexstatid')\n",
    "\n",
    "for k, v in seven.items():\n",
    "    idx = v[0] \n",
    "    print(\"{0:20} \\t {1} \\t{2} \\t {3} \\t {4}\".format(\n",
    "        lexwen[idx, 'doculect'], \n",
    "        lexwen[idx, 'IPA'], \n",
    "        lexwen[idx, 'turchinid'], \n",
    "        lexwen[idx, 'scaid'],\n",
    "        lexwen[idx, 'lexstatid']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is not much difference in the results for this very item, but you should not underestimate the different power of the methods, as we will see later on when running an evaluation analysis. For now, trust me that in general the results are quite different.\n",
    "\n",
    "Let us now run (for those who managed to install the python-igraph package) an additional analysis which was shown to yield even better results. Here, we do still use the \"lexstat\" approach, but we use \"infomap\" ([Rosvall and Bergstroem 2008](http://bibliography.lingpy.org?key=Rosvall2008)) as our cluster method. This method is network-based rather than agglomerative (as is LingPy's default), and was shown to yield consistently better results in combination with \"lexstat\" ([List, Greenhill, and Gray 2017](http://bibliography.lingpy.org?key=List2017c)). In order to avoid that we override the content of the column \"lexstatid\", we now pass a specific keyword, called `ref` (the \"reference\" of the output) and set its value to \"infomap\". We also choose a different threshold, the one we empirically determined from tests on different language families (see ibd. for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.cluster(method=\"lexstat\", threshold=0.55, ref=\"infomap\", cluster_method='infomap')\n",
    "for k, v in eight.items():\n",
    "    idx = v[0] \n",
    "    print(\"{0:20} \\t {1} \\t{2} \\t {3} \\t {4} \\t {5}\".format(\n",
    "        lex[idx, 'doculect'], \n",
    "        lex[idx, 'value'], \n",
    "        lex[idx, 'turchinid'], \n",
    "        lex[idx, 'scaid'],\n",
    "        lex[idx, 'lexstatid'],\n",
    "        lex[idx, 'infomap']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用informap计算温州方言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexwen.cluster(method=\"lexstat\", threshold=0.55, ref=\"infomap\", cluster_method='infomap')\n",
    "for k, v in seven.items():\n",
    "    idx = v[0] \n",
    "    print(\"{0:20} \\t {1} \\t{2} \\t {3} \\t {4} \\t {5}\".format(\n",
    "        lexwen[idx, 'doculect'], \n",
    "        lexwen[idx, 'IPA'], \n",
    "        lexwen[idx, 'turchinid'], \n",
    "        lexwen[idx, 'scaid'],\n",
    "        lexwen[idx, 'lexstatid'],\n",
    "        lexwen[idx, 'infomap']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, no improvement for \"eight\", but we will see later in detail, and for now, we just write the data to file, this time in plain text, without the additional information, but with the additional columns with our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:17:46,092 [INFO] Data has been written to file <east-polynesian-lexstat.tsv>.\n"
     ]
    }
   ],
   "source": [
    "lex.output('tsv', filename='east-polynesian-lexstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:33:21,548 [INFO] Data has been written to file <wenzhou-lexstat.tsv>.\n"
     ]
    }
   ],
   "source": [
    "lexwen.output('tsv', filename='wenzhou-lexstat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Aligning the Results\n",
    "\n",
    "One great advantage of LingPy is that alignments can also be directly computed from automatically inferred cognate sets. This is useful, first also for manually annotated cognate sets, as it saves a lot of work, since alignment algorithms come very close to human judgments, and it requires only minimal post-annotation by humans to correct the errors. Second, it is useful to check the data, as it makes transparent where the algorithm found the similarity that triggered a respective cognate decision.\n",
    "\n",
    "When carrying out alignment analyses, we use the `Alignments` class in LingPy which requires a word list as input as well as the column which contains the cognate sets which shall be aligned. We will use the \"infomap\" analysis for our automatic alignments, since this usually performs better than the other methods. This is done by specifying the keyword `ref` as \"infomap\" when calling the `Alignments` class. As a further important tweak, we first load the data into the `LexStat` class so that we have the inferred sound correspondences which will then be used to compute our alignments. For this purpose, we load the file `east-polynesian.bin.tsv` which stores the results of our permutation analysis and provides language-specific scores for all segments in the data (high scores indicating likely sound correspondences, low scores < 0 indicating non-corresponding sounds). We align using the normal progressive alignment, which is usually sufficient for smaller alignments and is slightly faster. When calling the alignment algorithm, we define the specific keyword `scoredict` and pass it the `lex.cscorer`, which stores the language-specific scoring functions for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = LexStat('east-polynesian.bin.tsv')\n",
    "alm = Alignments('east-polynesian-lexstat.tsv', ref='infomap', segments='tokens') # `ref` indicates the column with the cognate sets\n",
    "alm.align(method='progressive', scoredict=lex.cscorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexwen = LexStat('wenzhou.bin.tsv')\n",
    "almwen = Alignments('wenzhou-lexstat.tsv', ref='infomap', segments='tokens') # `ref` indicates the column with the cognate sets\n",
    "almwen.align(method='progressive', scoredict=lexwen.cscorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was not very spectacular, as we have not yet seen what happened. We can visualize the alignments from the command line by picking a particular cognate set and printing the alignments on screen. The alignments are added in a specific column called `alignments` as a default (but which can be modified by specifying another value with the keyword `alignments` passed to the initialization method for the `Alignments` class). Additionally, they can be retrieved using the `Alignments.get_msa` method - since multiple different alignment analyses can be stored in the object, the reference to a particular analysis must be passed. The following code illustrates how we can print a particular aligned cognate set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawaiian             \t Eight \t w\ta\tl\tu\n",
      "Mangareva            \t Eight \t v\ta\tr\tu\n",
      "Maori                \t Eight \t w\ta\tr\tu\n",
      "North_Marquesan      \t Eight \t v\ta\tʔ\tu\n",
      "Rapanui              \t Eight \t v\ta\tʔ\tu\n",
      "Ra’ivavae            \t Eight \t v\ta\tɢ\tu\n",
      "Rurutuan             \t Eight \t v\ta\tʔ\tu\n",
      "Sikaiana             \t Eight \t v\ta\tl\tu\n",
      "Tahitian             \t Eight \t v\ta\tʔ\tu\n",
      "Tuamotuan            \t Eight \t v\ta\tr\tu\n"
     ]
    }
   ],
   "source": [
    "msa = alm.get_msa('infomap')[1]\n",
    "for i, idx in enumerate(msa['ID']):\n",
    "    print(\n",
    "        '{0:20}'.format(msa['taxa'][i]),  \n",
    "        '\\t',\n",
    "        alm[idx, 'concept'],\n",
    "        '\\t',\n",
    "        '\\t'.join(msa['alignment'][i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乐清                   \t 一 \t -\ti\tɤ\t-\t-\n",
      "平阳                   \t 一 \t -\ti\tᴀ\t-\t-\n",
      "文成                   \t 一 \t -\ti\ta\t-\t-\n",
      "永嘉                   \t 一 \t -\ti\ta\t-\ti\n",
      "永嘉                   \t 一 \t -\t-\ti\t-\t-\n",
      "泰顺仕阳                 \t 一 \t ɕ\ti\tɪ\tʔ\t-\n",
      "泰顺仕阳                 \t 一 \t -\ti\tɪ\tʔ\t-\n",
      "泰顺罗阳                 \t 一 \t -\ti\tɛ\tʔ\t-\n",
      "洞头区                  \t 一 \t -\ti\te\t-\t-\n",
      "灵溪沪山                 \t 一 \t -\t-\te\t-\t-\n",
      "灵溪沪山                 \t 一 \t -\ti\tɛ\t-\t-\n",
      "灵溪顺昌                 \t 一 \t -\ti\te\t-\t-\n",
      "瑞安                   \t 一 \t -\t-\te\t-\t-\n",
      "瑞安                   \t 一 \t -\ti\ta\t-\t-\n",
      "鹿城                   \t 一 \t -\ti\ta\t-\ti\n"
     ]
    }
   ],
   "source": [
    "msawen = almwen.get_msa('infomap')[1]\n",
    "for i, idx in enumerate(msawen['ID']):\n",
    "    print(\n",
    "        '{0:20}'.format(msawen['taxa'][i]),  \n",
    "        '\\t',\n",
    "        almwen[idx, 'concept'],\n",
    "        '\\t',\n",
    "        '\\t'.join(msawen['alignment'][i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the eight, although this was not planned. But now let's quickly save the data to file, so that we can go on and inspect the findings further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:22:53,326 [INFO] Data has been written to file <east-polynesian-aligned.tsv>.\n"
     ]
    }
   ],
   "source": [
    "alm.output('tsv', filename='east-polynesian-aligned', ignore='all', prettify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:47:01,867 [INFO] Data has been written to file <wenzhou-aligned.tsv>.\n"
     ]
    }
   ],
   "source": [
    "almwen.output('tsv', filename='wenzhou-aligned', ignore='all', prettify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Inspecting Alignments with the EDICTOR\n",
    "\n",
    "The easiest way to inspect your alignments is to load them into the EDICTOR tool ([List 2017](http://bibliography.lingpy.org?key=List2017d)). In order to do so, just take the file ```east-polynesian-aligned.tsv```, open the EDICTOR at [http://edictor.digling.org](http://edictor.digling.org) and drag the file into the BROWSE button (or press the button and select the file). A red field will pop up and after pressing it, you can inspect your data in the EDICTOR.\n",
    "\n",
    "To lookup an alignment, just right-click in the field COGID for any numeric entry. If the cognate set is consisting of more than one word form, a popup window will appear in which you can inspect and manually edit the alignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alms](img/alms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Sound Correspondences\n",
    "\n",
    "EDICTOR offers a new module in which you can inspect the sound correspondence patterns of your data. To search those, just load your file into the EDICTOR tool, and click on ANALYZE->CORRESPONDENCE PATTERNS. If you then click the OK button, you will see the correspondence patterns for the aligned East Polynesian data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![correspondence patterns](img/corrs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Evaluation with LingPy\n",
    "\n",
    "#### 5.1 Manual Inspection of Differences\n",
    "\n",
    "If you want to manually inspect the differences after having computed automatic cognates, you can write data in LingPy to a textfile which easily contrasts the differences between experts' cognate judgments and automatic cognates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:57:46,401 [INFO] Data has been written to file <east-polynesian.diff>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.9433366501871404, 0.9227073959393681, 0.9329079941013704),\n",
       " (0.9101783453370696, 0.8872332528489478, 0.8985593448056826))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lingpy.evaluate.acd import diff\n",
    "wl = Wordlist('east-polynesian-lexstat.tsv')\n",
    "out = diff(wl, 'cogid', 'infomap', tofile=True, filename=\"east-polynesian\", pprint=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the file ```east-polynesian.diff```, you will see the following output:\n",
    "\n",
    "```text\n",
    "Concept: above, False Positives: no, False Negatives: yes\n",
    "Hawaiian       \tiluna\t   1\t   1\n",
    "Mangareva      \truŋa \t   1\t   1\n",
    "Maori          \tiruŋa\t   1\t   1\n",
    "North_Marquesan\tʔuna \t   1\t   1\n",
    "Rapanui        \truga \t   1\t   1\n",
    "Ra’ivavae      \tɢuʔa \t   1\t   1\n",
    "Tuamotuan      \truŋa \t   1\t   1\n",
    "Rurutuan       \tnuʔa \t   1\t   7\n",
    "Tahitian       \tniʔa \t   1\t   7\n",
    "#\n",
    "Concept: all, False Positives: yes, False Negatives: no\n",
    "Hawaiian       \tapau      \t   1\t   1\n",
    "Mangareva      \tkouroa    \t   2\t   2\n",
    "Maori          \tkatoa     \t   3\t   3\n",
    "Ra’ivavae      \tʔatoʔa    \t   3\t   3\n",
    "Rurutuan       \tpaːʔaːtoʔa\t   3\t   3\n",
    "Sikaiana       \tkatoa     \t   3\t   3\n",
    "Tahitian       \tatoʔa     \t   3\t   3\n",
    "Tuamotuan      \tkatoŋa    \t   3\t   3\n",
    "North_Marquesan\ttiatohu   \t   4\t   3\n",
    "Rapanui        \ttahi      \t   5\t   5\n",
    "#\n",
    "```\n",
    "\n",
    "This output shows you the expert cognate judgment in the first column, and the algorithmic judgments in the second colummn. Differences are easy to spot, as we follow consecutive numerical ordering, so if two numbers are not the same, it means the algorithm differs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Computing B-Cubed Scores\n",
    "\n",
    "LingPy has a couple of evaluation methods implemented, and since we have original expert cognate judgments from ABVD, we can compare our findings against theirs. Comparing cognate set accuracy is not necessarily a trivial problem, as we deal with cluster comparisons, which is a topic that was debated a lot in circles outside of linguistics, and it would lead us too far away if we discussed it in detail here now. For a linguistic viewpoint with a brief working example of our preferred method, the B-Cubed scores (see [Hauer and Kondrak 2011](http://bibliography.lingpy.org?key=Hauer2011), [Bagga and Baldwin 1998](http://bibliography.lingpy.org?key=Bagga1998), and [Amigo et al. 2009](http://bibliography.lingpy.org?key=Amigo2009)), see List, Greenhill, and Gray (2017). What you need to know, however, is that evaluation in NLP circles usually comes along with the concepts of *precision*, *recall*, and *f-score*. Initially, I found them rather difficult to grasp, as historical linguists usually think in terms of false positives and false negatives. In order to understand the idea, one should think that an algorithm for cognate detection can basically do two things either right or wrong: it could cluster words which are not cognate, or it could fail to cluster words as cognate. In the first case, we would measure this in terms of precision, by counting, how often the algorithm proposes correct or incorrect answers, and in the latter case, we measure the proportion of cognate sets which are missed. In the B-Cubed measure we use, this translates roughly to a measure of false/true positives vs. false/true negatives, but it is not entirely the same. The f-score computes the harmonic mean, which summarizes both values, and we usually want to improve the f-score and we use it to compare different algorithms with each other. \n",
    "\n",
    "Let's start and do this comparison now, by loading the respective functions from the LingPy evaluation module, and computing precision, recall, and f-scores for all our different automatically inferred cognate sets with respect to the gold standard. The gold standard is located in the column `COGID` of the input file, so we need to name this when comparing with any of the other columns (like `LEXSTATID`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.evaluate.acd import bcubes\n",
    "from lingpy import *\n",
    "wl = Wordlist('east-polynesian-lexstat.tsv')\n",
    "\n",
    "for res in ['turchinid', 'editid', 'scaid', 'lexstatid', 'infomap']:\n",
    "    print('{0:10}\\t{1[0]:.4f}\\t{1[1]:.4f}\\t{1[2]:.4f}'.format(\n",
    "        res,\n",
    "        bcubes(wl, 'cogid', res, pprint=False)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that the \"infomap\" method is, in fact, working slightly better than the normal \"lexstat\" method, and you can also see how deep the difference between the correspondence-informed methods and the other methods is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Exporting Data\n",
    "\n",
    "It is clear that for many of those who consult automatic cognate detection, they use the methods in order to be able to do more with the data afterwards. LingPy so far supports quite a few different ways to write your data to file for further use in other software packages. A complete integration of `Nexus` files which transport all information which might be relevant for BEAST, however, does not exist yet (but will be added at some point sooner than later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Nexus-Export\n",
    "\n",
    "Nexus export is straightforward in LingPy, and currently, two formats, MrBayes and BEAST are supported. The following code will export our latest wordlist to Nexus in MrBayes format, using the expert cognate judgments for export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.convert.strings import write_nexus\n",
    "nexus = write_nexus(wl, ref='cogid', mode='mrbayes', filename='east-polynesian-mb.nex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to export the automatic cognate judgments to BEAST nexus format, you can do so by changing the \"mode\" keyword and the \"ref\" keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nexus = write_nexus(wl, ref=\"lexstatid\", mode=\"beast\", filename='east-polynesian-beast.nex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you want BEAST to use concept-specific rates instead of general rates for all data, you can do so selecting \"beastwords\" as your mode of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nexus = write_nexus(wl, ref=\"lexstatid\", mode=\"beastwords\", filename=\"east-polynesian-beastw.nex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Distances and Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate distances which would be interesting for packages like SplitsTree (Huson 1998), or also Phylip ([Felsenstein 2005](http://bibliography.lingpy.org?key=Felsenstein2005). For this, you need to be careful, however, since distances can be computed in different ways, and you can choose from a multitude of different distances, and they are not (yet) all documented. The distance calculation as a default counts, how many cognates there are for all concepts between each language pair, so in some way, this tries to mimick Swadesh's original idea of distances or similarities between languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:58:05,438 [INFO] Successfully calculated dst.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10\n",
      "Hawaiian   0.0000 0.3448 0.3905 0.3188 0.3873 0.4792 0.3810 0.4948 0.4450 0.3015\n",
      "Mangareva  0.3448 0.0000 0.3498 0.2800 0.3467 0.4497 0.3941 0.4211 0.4356 0.2769\n",
      "Maori      0.3905 0.3498 0.0000 0.3527 0.4020 0.4948 0.4190 0.4433 0.4593 0.3166\n",
      "North_Marqu 0.3188 0.2800 0.3527 0.0000 0.3812 0.4762 0.4203 0.4010 0.4541 0.2893\n",
      "Rapanui    0.3873 0.3467 0.4020 0.3812 0.0000 0.4947 0.4559 0.4241 0.4608 0.3316\n",
      "Ra’ivavae  0.4792 0.4497 0.4948 0.4762 0.4947 0.0000 0.2448 0.6313 0.2199 0.4565\n",
      "Rurutuan   0.3810 0.3941 0.4190 0.4203 0.4559 0.2448 0.0000 0.5619 0.1340 0.3467\n",
      "Sikaiana   0.4948 0.4211 0.4433 0.4010 0.4241 0.6313 0.5619 0.0000 0.5928 0.3690\n",
      "Tahitian   0.4450 0.4356 0.4593 0.4541 0.4608 0.2199 0.1340 0.5928 0.0000 0.3869\n",
      "Tuamotuan  0.3015 0.2769 0.3166 0.2893 0.3316 0.4565 0.3467 0.3690 0.3869 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from lingpy.convert.strings import matrix2dst\n",
    "\n",
    "dst = matrix2dst(wl.get_distances(ref='infomap', mode='swadesh'), wl.taxa)\n",
    "with io.open('east-polynesian.dst', 'w', encoding='utf8') as fp:\n",
    "    fp.write(dst)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 03:08:14,472 [INFO] Successfully calculated dst.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11\n",
      "乐清         0.0000 0.0253 0.0316 0.0547 0.1434 0.0570 0.2445 0.0558 0.2497 0.0679 0.0836\n",
      "平阳         0.0253 0.0000 0.0299 0.0375 0.1553 0.0688 0.2474 0.0508 0.2523 0.0540 0.0769\n",
      "文成         0.0316 0.0299 0.0000 0.0541 0.1480 0.0669 0.2453 0.0583 0.2482 0.0654 0.0845\n",
      "永嘉         0.0547 0.0375 0.0541 0.0000 0.1623 0.0798 0.2442 0.0540 0.2479 0.0640 0.0577\n",
      "泰顺仕阳       0.1434 0.1553 0.1480 0.1623 0.0000 0.1459 0.1977 0.1587 0.2023 0.1653 0.1726\n",
      "泰顺罗阳       0.0570 0.0688 0.0669 0.0798 0.1459 0.0000 0.2395 0.0564 0.2461 0.0920 0.1109\n",
      "洞头区        0.2445 0.2474 0.2453 0.2442 0.1977 0.2395 0.0000 0.2230 0.0243 0.2597 0.2463\n",
      "灵溪沪山       0.0558 0.0508 0.0583 0.0540 0.1587 0.0564 0.2230 0.0000 0.2263 0.0763 0.0904\n",
      "灵溪顺昌       0.2497 0.2523 0.2482 0.2479 0.2023 0.2461 0.0243 0.2263 0.0000 0.2646 0.2516\n",
      "瑞安         0.0679 0.0540 0.0654 0.0640 0.1653 0.0920 0.2597 0.0763 0.2646 0.0000 0.0486\n",
      "鹿城         0.0836 0.0769 0.0845 0.0577 0.1726 0.1109 0.2463 0.0904 0.2516 0.0486 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from lingpy.convert.strings import matrix2dst\n",
    "wlwen = Wordlist('wenzhou-lexstat.tsv')\n",
    "dst = matrix2dst(wlwen.get_distances(ref='infomap', mode='swadesh'), wlwen.taxa)\n",
    "with io.open('wenzhou.dst', 'w', encoding='utf8') as fp:\n",
    "    fp.write(dst)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format follows strictly the Phylip distance format which also cuts off all language names longer than 10 characters (but there are ways to modify this, I can't show them now).\n",
    "\n",
    "\n",
    "As a final experiment, let us create a tree from the distances, using the simple Neighbor-Joining algorithm, and then print this tree to screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 02:58:23,628 [INFO] Successfully calculated tree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    /-Ra’ivavae\n",
      "          /edge.1--|\n",
      "         |         |          /-Rurutuan\n",
      "         |          \\edge.0--|\n",
      "-root----|                    \\-Tahitian\n",
      "         |\n",
      "         |          /-Sikaiana\n",
      "          \\edge.7--|\n",
      "                   |          /-Rapanui\n",
      "                    \\edge.6--|\n",
      "                             |          /-Maori\n",
      "                              \\edge.5--|\n",
      "                                       |          /-Hawaiian\n",
      "                                        \\edge.4--|\n",
      "                                                 |          /-North_Marquesan\n",
      "                                                  \\edge.3--|\n",
      "                                                           |          /-Mangareva\n",
      "                                                            \\edge.2--|\n",
      "                                                                      \\-Tuamotuan\n"
     ]
    }
   ],
   "source": [
    "tree = Tree(wl.get_tree(ref='infomap', tree_calc='upgma', force=True))\n",
    "print(tree.asciiArt())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 03:08:37,129 [INFO] Successfully calculated tree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    /-洞头区\n",
      "          /edge.0--|\n",
      "         |          \\-灵溪顺昌\n",
      "-root----|\n",
      "         |          /-泰顺仕阳\n",
      "         |         |\n",
      "          \\edge.8--|                    /-瑞安\n",
      "                   |          /edge.1--|\n",
      "                   |         |          \\-鹿城\n",
      "                    \\edge.7--|\n",
      "                             |          /-泰顺罗阳\n",
      "                              \\edge.6--|\n",
      "                                       |          /-灵溪沪山\n",
      "                                        \\edge.5--|\n",
      "                                                 |          /-永嘉\n",
      "                                                  \\edge.4--|\n",
      "                                                           |          /-文成\n",
      "                                                            \\edge.3--|\n",
      "                                                                     |          /-乐清\n",
      "                                                                      \\edge.2--|\n",
      "                                                                                \\-平阳\n"
     ]
    }
   ],
   "source": [
    "tree = Tree(wlwen.get_tree(ref='infomap', tree_calc='upgma', force=True))\n",
    "print(tree.asciiArt())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not up to me to judge how good this tree is, and it may also be wrongly rooted in the display. But you can see that LingPy can also handle classical tree formats. Although we do not plan to make LingPy a concurrence for tree inference packages, we find it useful to offer Neighbor-joining and UPGMA just to make it easier for users to quickly evaluate their analyses.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 CLDF Export\n",
    "\n",
    "The Cross-Linguistic Data Formats initiative ([Forkel et al. 2017](http://bibliography.lingpy.org?key=Forkel2017a)) provides standardized formats for the sharing of data amenable for cross-linguistic comparison. LingPy now also offers the possibility to export to CLDF as well as to read CLDF files. Since CLDF is more explicit and powerful than LingPy's file-formats, you can add additional data, like your sources in form of BibTex files. We have prepared a BibTex file along with this tutorial and pass it to the algorithm, so that it becomes included into the CLDF-package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.convert.cldf import to_cldf\n",
    "from lingpy.basic.wordlist import from_cldf\n",
    "to_cldf(wl, path='cldf', source_path='polynesian.bib', source='source', form='form')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been exported, you can easily import it back, using the ```from_cldf``` function. Just make sure to specify the metadata file in JSON format as the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = from_cldf(\"cldf/Wordlist-metadata.json\")\n",
    "print(wl.height, wl.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<ul>\n",
    "<li>Amigó, E., J. Gonzalo, J. Artiles, and F. Verdejo (2009): <strong>A comparison of extrinsic clustering evaluation metrics based on formal constraints</strong>. <em>Information Retrieval</em> 12.4. 461-486. </li>\n",
    "<li>Bagga, A. and B. Baldwin (1998): <strong>Entity-based cross-document coreferencing using the vector space model</strong>. In: <strong>Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</strong>. Association of Computational Linguistics 79-85.</li>\n",
    "<li>Bouckaert, R., J. Heled, D. Kühnert, T. Vaughan, C.-H. Wu, D. Xie, M. Suchard, A. Rambaut, and A. Drummond (2014): <strong>BEAST 2: A Software Platform for Bayesian Evolutionary Analysis</strong>. <em>PLoS Computational Biology</em> 10.4. e1003537. </li>\n",
    "<li>Brown, C., E. Holman, and S. Wichmann (2013): <strong>Sound correspondences in the world’s languages</strong>. <em>Language</em> 89.1. 4-29. </li>\n",
    "<li>Cs\\’ardi, G. and T. Nepusz (2006): <strong>The igraph software package for complex network research</strong>. <em>InterJournal. Complex Systems</em> .1695. . </li>\n",
    "<li>Dolgopolsky, A. (1964): <strong>Gipoteza drevnejšego rodstva jazykovych semej Severnoj Evrazii s verojatnostej točky zrenija</strong> [A probabilistic hypothesis concering the oldest relationships among the language families of Northern Eurasia]. <em>Voprosy Jazykoznanija</em> 2. 53-63. </li>\n",
    "<li>Downey, S., B. Hallmark, M. Cox, P. Norquest, and S. Lansing (2008): <strong>Computational feature-sensitive reconstruction of language relationships: developing the ALINE distance for comparative historical linguistic reconstruction</strong>. <em>Journal of Quantitative Linguistics</em> 15.4. 340-369. </li>\n",
    "<li>Felsenstein, J. (2005): <strong>PHYLIP (Phylogeny Inference Package)</strong>. </li>\n",
    "<li>Forkel, R., S. Greenhill, and J.-M. List (2017): <strong>Cross-Linguistic Data Formats (CLDF)</strong>. Max Planck Institute for the Science of Human History: Jena.</li>\n",
    "<li>Greenhill, S., R. Blust, and R. Gray (2008): <strong>The Austronesian Basic Vocabulary Database: From bioinformatics to lexomics</strong>. <em>Evolutionary Bioinformatics</em> 4. 271-283. </li>\n",
    "<li>Hammarström, H., R. Forkel, and M. Haspelmath (2017): <strong>Glottolog</strong>. Max Planck Institute for Evolutionary Anthropology: Leipzig. </li>\n",
    "<li>Hauer, B. and G. Kondrak (2011): <strong>Clustering semantically equivalent words into cognate sets in multilingual lists</strong>. In: <strong>Proceedings of the 5th International Joint Conference on Natural Language Processing</strong>. AFNLP 865-873.</li>\n",
    "<li>Huson, D. (1998): <strong>SplitsTree: analyzing and visualizing evolutionary data</strong>. <em>Bioinformatics</em> 14.1. 68-73. </li>\n",
    "<li>Levenshtein, V. (1965): <strong>Dvoičnye kody s ispravleniem vypadenij, vstavok i zameščenij simvolov</strong> [Binary codes with correction of deletions, insertions and replacements]. <em>Doklady Akademij Nauk SSSR</em> 163.4. 845-848. </li>\n",
    "<li>List, J.-M. (2012): <strong>SCA. Phonetic alignment based on sound classes</strong>. In: Slavkovik, M. and D. Lassiter (eds.): <strong>New directions in logic, language, and computation</strong>. Springer: Berlin and Heidelberg. 32-51.</li>\n",
    "<li>List, J.-M. (2012): <strong>LexStat. Automatic detection of cognates in multilingual wordlists</strong>. In: <strong>Proceedings of the EACL 2012 Joint Workshop of Visualization of Linguistic Patterns and Uncovering Language History from Multilingual Resources</strong>. 117-125.</li>\n",
    "<li>List, J.-M. (2014): <strong>Investigating the impact of sample size on cognate detection</strong>. <em>Journal of Language Relationship</em> 11. 91-101. </li>\n",
    "<li>List, J.-M. (2014): <strong>Sequence comparison in historical linguistics</strong>. Düsseldorf University Press: Düsseldorf.</li>\n",
    "<li>List, J.-M., M. Cysouw, and R. Forkel (2016): <strong>Concepticon. A resource for the linking of concept lists</strong>. In: <strong>Proceedings of the Tenth International Conference on Language Resources and Evaluation</strong>. 2393-2400.</li>\n",
    "<li>List, J.-M., P. Lopez, and E. Bapteste (2016): <strong>Using sequence similarity networks to identify partial cognates in multilingual wordlists</strong>. In: <strong>Proceedings of the Association of Computational Linguistics 2016 (Volume 2: Short Papers)</strong>. Association of Computational Linguistics 599-605.</li>\n",
    "<li>List, J.-M. (2017): <strong>Introduction to computer-assisted language comparison</strong> [Einführung in den computergestützten Sprachvergleich]. Institute of Linguistics: Nankai University (Tianjin, China).</li>\n",
    "<li>List, J.-M., S. Greenhill, and R. Gray (2017): <strong>The potential of automatic word comparison for historical linguistics</strong>. <em>PLOS ONE</em> 12.1. 1-18. </li>\n",
    "<li>List, J.-M. (2017): <strong>A web-based interactive tool for creating, inspecting, editing, and publishing etymological datasets</strong>. In: <strong>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. System Demonstrations</strong>. 9-12.</li>\n",
    "<li>List, J.-M. and R. Forkel (2017): <strong>LingPy. A Python library for historical linguistics</strong>. Max Planck Institute for the Science of Human History: Jena. </li>\n",
    "<li>Moran, S. and M. Cysouw (2017): <strong>The Unicode Cookbook for Linguists: Managing\n",
    "writing systems using orthography profiles</strong>. Zenodo: Zürich.</li>\n",
    "<li>Notredame, C., D. Higgins, and J. Heringa (2000): <strong>T-Coffee</strong><strong>. A novel method for fast and accurate multiple sequence alignment</strong>. <em>Journal of Molecular Biology</em> 302. 205-217. </li>\n",
    "<li>Rosvall, M. and C. Bergstrom (2008): <strong>Maps of random walks on complex networks reveal community structure</strong>. <em>Proceedings of the National Academy of Sciences</em> 105.4. 1118-1123. </li>\n",
    "<li>Saitou, N. and M. Nei (1987): <strong>The neighbor-joining method: A new method for reconstructing phylogenetic trees</strong>. <em>Molecular Biology and Evolution</em> 4.4. 406-425. </li>\n",
    "<li>Sokal, R. and C. Michener (1958): <strong>A statistical method for evaluating systematic relationships</strong>. <em>University of Kansas Scientific Bulletin</em> 28. 1409-1438. </li>\n",
    "<li>Turchin, P., I. Peiros, and M. Gell-Mann (2010): <strong>Analyzing genetic connections between languages by matching consonant classes</strong>. <em>Journal of Language Relationship</em> 3. 117-126. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子 tokens： ['永嘉县', '旧属', '温州府', '东濒', '东海', '西接', '青田']\n",
      "\n",
      "当前 Query token： 东濒\n",
      "\n",
      "注意力权重（对每个 token 的关注度）：\n",
      "  永嘉县   ->  0.0004\n",
      "  旧属    ->  0.0981\n",
      "  温州府   ->  0.0014\n",
      "  东濒    ->  0.3447\n",
      "  东海    ->  0.0002\n",
      "  西接    ->  0.5384\n",
      "  青田    ->  0.0168\n",
      "\n",
      "输出向量 output_vector 的形状： (1, 8)\n",
      "（这是“东濒”在 self-attention 后的新表征向量）\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 为了可重复：固定随机种子\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# 1. 定义一句“永嘉县志风格”的句子和 token\n",
    "tokens = [\"永嘉县\", \"旧属\", \"温州府\", \"东濒\", \"东海\", \"西接\", \"青田\"]\n",
    "n_tokens = len(tokens)\n",
    "\n",
    "# 2. 假设每个 token 已经有一个 d_model 维的 embedding\n",
    "d_model = 8   # 向量维度（这里只是示范，实际可能 512 / 768 / 1024 等）\n",
    "X = rng.normal(size=(n_tokens, d_model))  # (n_tokens, d_model)\n",
    "\n",
    "# 3. 随机初始化 Q, K, V 的线性变换矩阵\n",
    "#   实际中这些是可训练参数，这里只是教学用随机矩阵\n",
    "d_k = d_model\n",
    "d_v = d_model\n",
    "\n",
    "W_Q = rng.normal(size=(d_model, d_k))\n",
    "W_K = rng.normal(size=(d_model, d_k))\n",
    "W_V = rng.normal(size=(d_model, d_v))\n",
    "\n",
    "# 4. 计算 Q, K, V 矩阵\n",
    "#   Q, K, V 形状都是 (n_tokens, d_*)\n",
    "Q = X @ W_Q   # Query\n",
    "K = X @ W_K   # Key\n",
    "V = X @ W_V   # Value\n",
    "\n",
    "# 5. 定义一个 softmax 函数（对最后一维做归一化）\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)  # 防止数值溢出\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# 6. 选定一个关注的 token：这里选“东濒”，下标 3\n",
    "idx = 3  # \"东濒\"\n",
    "\n",
    "q = Q[idx:idx+1]         # 形状 (1, d_k)，只取这一行\n",
    "scores = q @ K.T         # 形状 (1, n_tokens)，分别与每个 K 做点积\n",
    "scores_scaled = scores / np.sqrt(d_k)  # 除以 sqrt(d_k)\n",
    "\n",
    "# 7. 对 scores 做 softmax，得到对每个 token 的注意力权重\n",
    "attn_weights = softmax(scores_scaled, axis=-1)  # (1, n_tokens)\n",
    "\n",
    "# 8. 用注意力权重对所有 V 做加权求和，得到“东濒”的新表示\n",
    "output_vector = attn_weights @ V  # 形状 (1, d_v)\n",
    "\n",
    "# 9. 打印结果，便于教学展示\n",
    "print(\"句子 tokens：\", tokens)\n",
    "print(\"\\n当前 Query token：\", tokens[idx])\n",
    "print(\"\\n注意力权重（对每个 token 的关注度）：\")\n",
    "for t, w in zip(tokens, attn_weights[0]):\n",
    "    print(f\"  {t:4s}  ->  {w:.4f}\")\n",
    "\n",
    "print(\"\\n输出向量 output_vector 的形状：\", output_vector.shape)\n",
    "print(\"（这是“东濒”在 self-attention 后的新表征向量）\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
